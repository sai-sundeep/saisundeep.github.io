[
  {
    "objectID": "posts/Regression/index.html",
    "href": "posts/Regression/index.html",
    "title": "Linear and Non-Linear Regression",
    "section": "",
    "text": "Code\n#Import Packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nsns.set_theme(style=\"darkgrid\")\n\n\n\n\nCode\ndiamonds = sns.load_dataset('diamonds')\nprint(diamonds.shape)\nprint(diamonds.describe())\nprint(diamonds.head(10))\n\n\n(53940, 10)\n              carat         depth         table         price             x  \\\ncount  53940.000000  53940.000000  53940.000000  53940.000000  53940.000000   \nmean       0.797940     61.749405     57.457184   3932.799722      5.731157   \nstd        0.474011      1.432621      2.234491   3989.439738      1.121761   \nmin        0.200000     43.000000     43.000000    326.000000      0.000000   \n25%        0.400000     61.000000     56.000000    950.000000      4.710000   \n50%        0.700000     61.800000     57.000000   2401.000000      5.700000   \n75%        1.040000     62.500000     59.000000   5324.250000      6.540000   \nmax        5.010000     79.000000     95.000000  18823.000000     10.740000   \n\n                  y             z  \ncount  53940.000000  53940.000000  \nmean       5.734526      3.538734  \nstd        1.142135      0.705699  \nmin        0.000000      0.000000  \n25%        4.720000      2.910000  \n50%        5.710000      3.530000  \n75%        6.540000      4.040000  \nmax       58.900000     31.800000  \n   carat        cut color clarity  depth  table  price     x     y     z\n0   0.23      Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n1   0.21    Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n2   0.23       Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n3   0.29    Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n4   0.31       Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75\n5   0.24  Very Good     J    VVS2   62.8   57.0    336  3.94  3.96  2.48\n6   0.24  Very Good     I    VVS1   62.3   57.0    336  3.95  3.98  2.47\n7   0.26  Very Good     H     SI1   61.9   55.0    337  4.07  4.11  2.53\n8   0.22       Fair     E     VS2   65.1   61.0    337  3.87  3.78  2.49\n9   0.23  Very Good     H     VS1   59.4   61.0    338  4.00  4.05  2.39\n\n\n\n\nCode\nsns.histplot(data=diamonds, x='price', kde=True)\nplt.title('Diamonds Dataset - Price Distribution')\nplt.ylabel(\"# of sales\")\nplt.xlabel(\"Price\")\nplt.grid('both')\nplt.show()\n\n\n\n\n\n\n\nCode\nsns.countplot(data=diamonds, y='cut', order=diamonds['cut'].value_counts().index,\n              palette=sns.color_palette('flare', 10))\nplt.title('Diamonds Dataset - sales by cut type')\nplt.xlabel('# of Sales')\nplt.ylabel('Cut Type')\nplt.grid('both')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nCode\nsns.countplot(data=diamonds, y='clarity', \n              order = diamonds['clarity'].value_counts(ascending=True).index,\n              palette=sns.color_palette('flare', 10))\nplt.xlabel('# of Sales')\nplt.ylabel('Clarity Type')\nplt.title('Diamonds Dataset - Sales by Clarity Type')\nplt.grid('both')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nCode\nsns.countplot(data=diamonds, \n              x = 'color', \n              order = diamonds['color'].value_counts(ascending=False).index,\n              palette=sns.color_palette('flare', 10))\nplt.ylabel('# of Sales')\nplt.xlabel('Color')\nplt.grid('both')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nCode\nsns.kdeplot(diamonds, x='price', hue='color', common_norm=False)\nplt.show()\n\n\n\n\n\n\n\nCode\ndiamonds['clarity'].value_counts(ascending=False)\n\n\nclarity\nSI1     13065\nVS2     12258\nSI2      9194\nVS1      8171\nVVS2     5066\nVVS1     3655\nIF       1790\nI1        741\nName: count, dtype: int64\n\n\n\n\nCode\ndiamonds['color'].value_counts(ascending=False)\n\n\ncolor\nG    11292\nE     9797\nF     9542\nH     8304\nD     6775\nI     5422\nJ     2808\nName: count, dtype: int64\n\n\n\n\nCode\ndiamonds['cut'].value_counts(ascending=False)\n\n\ncut\nIdeal        21551\nPremium      13791\nVery Good    12082\nGood          4906\nFair          1610\nName: count, dtype: int64\n\n\n\n\nCode\n# Analyzing distribution of numeric features with histogram plot\nsns.set_theme(style=\"whitegrid\")\ndiamonds.hist(bins=50, figsize=(12, 8))\nplt.show()\n\n\n\n\n\n\n\nCode\ncorr_matrix = diamonds.select_dtypes(np.number).corr()\n\n\n\n\nCode\ncorr_matrix['price'].sort_values(ascending=False)\n\n\nprice    1.000000\ncarat    0.921591\nx        0.884435\ny        0.865421\nz        0.861249\ntable    0.127134\ndepth   -0.010647\nName: price, dtype: float64\n\n\n\n\nCode\nfrom pandas.plotting import scatter_matrix\nscatter_matrix(diamonds[['price', 'carat', 'x', 'y', 'z', 'table', 'depth']], figsize=(15, 10))\nplt.show()\n\n\n\n\n\n\n\nCode\nfrom prettytable import PrettyTable\ndef pretty_printing_function(correlation_name, row_values, column_names):\n    summary_table = PrettyTable()\n    summary_table.title = f\"{correlation_name} Correlation Matrix for the tute1 dataset\"\n    column_names = [f'Feature({chr(0x2193)})/({chr(0x2192)})'] + column_names\n    summary_table.field_names = column_names\n    for i in range(len(row_values)):\n        row_index = column_names[i+1]\n        row_values[i] = [row_index] + row_values[i]\n        summary_table.add_row(row_values[i])\n    print(summary_table)\n\n\ndef calc_pearson_corr(x, y, N):\n    numerator_sum = 0.0; denomnator1_sum = 0.0; denomnator2_sum = 0.0\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    for i in range(N):\n        x_diff = x[i] - x_mean\n        y_diff = y[i] - y_mean\n        numerator_sum += (x_diff * y_diff)\n        denomnator1_sum += np.power(x_diff, 2)\n        denomnator2_sum += np.power(y_diff, 2)\n    pearson_corrcoef = numerator_sum/(np.sqrt(denomnator1_sum)*np.sqrt(denomnator2_sum))\n    return round(pearson_corrcoef, 2)\n\n\ndef calc_partial_corr(x, y, z):\n    r_xy = calc_pearson_corr(x, y, len(diamonds))\n    r_xz = calc_pearson_corr(x, z, len(diamonds))\n    r_yz = calc_pearson_corr(y, z, len(diamonds))\n    partial_corr = (r_xy - (r_xz*r_yz)) / (np.sqrt(1 - r_xz**2) * np.sqrt(1 - r_yz**2))\n    return round(partial_corr, 2)\n\n\ndiamonds_numeric = diamonds.select_dtypes(np.number)\ndef calc_partial_correlation():\n    summary_df = pd.DataFrame(columns = diamonds_numeric.columns, index = diamonds_numeric.columns)\n    for col1 in diamonds_numeric.columns:\n        for col2 in diamonds_numeric.columns:\n            if col1 == col2:\n                summary_df.loc[col1, col2] = 1.0\n            else:\n                other_columns = list(set(diamonds_numeric.columns) - set([col1, col2]))\n                for col3 in other_columns:\n                    summary_df.loc[col1, col2] = calc_partial_corr(diamonds_numeric[col1], diamonds_numeric[col2], diamonds_numeric[col3])\n    pretty_printing_function(\"Partial\", summary_df.values.tolist(), column_names=list(summary_df.columns))\n    return summary_df\n\ncalc_partial_correlation()\n\n\n+------------------------------------------------------------------------+\n|            Partial Correlation Matrix for the tute1 dataset            |\n+----------------+-------+-------+-------+-------+-------+-------+-------+\n| Feature(↓)/(→) | carat | depth | table | price |   x   |   y   |   z   |\n+----------------+-------+-------+-------+-------+-------+-------+-------+\n|     carat      |  1.0  | -0.18 |  0.12 |  0.65 |  0.77 |  0.49 | -0.01 |\n|     depth      | -0.18 |  1.0  | -0.32 | -0.17 | -0.48 | -0.37 |  0.49 |\n|     table      |  0.12 | -0.32 |  1.0  |  0.0  |  0.23 |  0.12 | -0.18 |\n|     price      |  0.65 | -0.17 |  0.0  |  1.0  |  0.37 |  0.33 |  0.06 |\n|       x        |  0.77 | -0.48 |  0.23 |  0.37 |  1.0  |  0.64 |  0.97 |\n|       y        |  0.49 | -0.37 |  0.12 |  0.33 |  0.64 |  1.0  |  0.15 |\n|       z        | -0.01 |  0.49 | -0.18 |  0.06 |  0.97 |  0.15 |  1.0  |\n+----------------+-------+-------+-------+-------+-------+-------+-------+\n\n\n\n\n\n\n\n\n\ncarat\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\ncarat\n1.0\n-0.18\n0.12\n0.65\n0.77\n0.49\n-0.01\n\n\ndepth\n-0.18\n1.0\n-0.32\n-0.17\n-0.48\n-0.37\n0.49\n\n\ntable\n0.12\n-0.32\n1.0\n0.0\n0.23\n0.12\n-0.18\n\n\nprice\n0.65\n-0.17\n0.0\n1.0\n0.37\n0.33\n0.06\n\n\nx\n0.77\n-0.48\n0.23\n0.37\n1.0\n0.64\n0.97\n\n\ny\n0.49\n-0.37\n0.12\n0.33\n0.64\n1.0\n0.15\n\n\nz\n-0.01\n0.49\n-0.18\n0.06\n0.97\n0.15\n1.0\n\n\n\n\n\n\n\n\n\nCode\ndiamonds.plot(kind=\"scatter\", x=\"price\", y=\"carat\",\n             alpha=0.07, grid=True)\nplt.show()\n\n\n\n\n\n\n\nCode\ndiamonds.shape\ndiamonds.head(10)\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n0.23\nIdeal\nE\nSI2\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n\n\n1\n0.21\nPremium\nE\nSI1\n59.8\n61.0\n326\n3.89\n3.84\n2.31\n\n\n2\n0.23\nGood\nE\nVS1\n56.9\n65.0\n327\n4.05\n4.07\n2.31\n\n\n3\n0.29\nPremium\nI\nVS2\n62.4\n58.0\n334\n4.20\n4.23\n2.63\n\n\n4\n0.31\nGood\nJ\nSI2\n63.3\n58.0\n335\n4.34\n4.35\n2.75\n\n\n5\n0.24\nVery Good\nJ\nVVS2\n62.8\n57.0\n336\n3.94\n3.96\n2.48\n\n\n6\n0.24\nVery Good\nI\nVVS1\n62.3\n57.0\n336\n3.95\n3.98\n2.47\n\n\n7\n0.26\nVery Good\nH\nSI1\n61.9\n55.0\n337\n4.07\n4.11\n2.53\n\n\n8\n0.22\nFair\nE\nVS2\n65.1\n61.0\n337\n3.87\n3.78\n2.49\n\n\n9\n0.23\nVery Good\nH\nVS1\n59.4\n61.0\n338\n4.00\n4.05\n2.39\n\n\n\n\n\n\n\n\n\nCode\n# One-hot encode the categorical variabled before feeding into linear model\nfrom sklearn.preprocessing import OneHotEncoder\n\ndiamonds_cut = diamonds[['cut']]\n\nenc = OneHotEncoder(handle_unknown='ignore')\ndiamonds_cuts_onehot = enc.fit_transform(diamonds_cut)\n\n\n\n\nCode\nenc.categories_\n\n\n[array(['Fair', 'Good', 'Ideal', 'Premium', 'Very Good'], dtype=object)]\n\n\n\n\nCode\nenc.get_feature_names_out()\n\n\narray(['cut_Fair', 'cut_Good', 'cut_Ideal', 'cut_Premium',\n       'cut_Very Good'], dtype=object)\n\n\n\n\nCode\ndiamonds_cuts_onehot.toarray()\n\n\narray([[0., 0., 1., 0., 0.],\n       [0., 0., 0., 1., 0.],\n       [0., 1., 0., 0., 0.],\n       ...,\n       [0., 0., 0., 0., 1.],\n       [0., 0., 0., 1., 0.],\n       [0., 0., 1., 0., 0.]])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer\n\ncategorical_features = ['cut', 'color', 'clarity']\nnumeric_features = ['x', 'y', 'z', 'carat', 'depth', 'table']\n\nnumeric_transformer = make_pipeline(SimpleImputer(strategy='median'), StandardScaler())\n\npreprocessor = ColumnTransformer(\n    [\n        ('num', numeric_transformer, numeric_features),\n        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),\n    ],\n    verbose_feature_names_out=False\n)\n\n\n\n\nCode\ndiamonds_features, diamonds_price = diamonds.loc[:, diamonds.columns != 'price'], diamonds['price']\n\n\n\n\nCode\ndiamonds_features.shape\n\n\n(53940, 9)\n\n\n\n\nCode\ndiamonds_price.shape\n\n\n(53940,)\n\n\n\n\nCode\ndiamonds_features.head()\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nx\ny\nz\n\n\n\n\n0\n0.23\nIdeal\nE\nSI2\n61.5\n55.0\n3.95\n3.98\n2.43\n\n\n1\n0.21\nPremium\nE\nSI1\n59.8\n61.0\n3.89\n3.84\n2.31\n\n\n2\n0.23\nGood\nE\nVS1\n56.9\n65.0\n4.05\n4.07\n2.31\n\n\n3\n0.29\nPremium\nI\nVS2\n62.4\n58.0\n4.20\n4.23\n2.63\n\n\n4\n0.31\nGood\nJ\nSI2\n63.3\n58.0\n4.34\n4.35\n2.75\n\n\n\n\n\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\ndiamonds_features_train, diamonds_features_test, diamonds_price_train, diamonds_price_test = train_test_split(diamonds_features, diamonds_price, test_size=0.25)\n\n#print(diamonds_features_train.head(), diamonds_price_train.head())\n\n\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = make_pipeline(preprocessor, LinearRegression())\nlin_reg.fit(diamonds_features_train, diamonds_price_train)\n\n\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('standardscaler',\n                                                                   StandardScaler())]),\n                                                  ['x', 'y', 'z', 'carat',\n                                                   'depth', 'table']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse_output=False),\n                                                  ['cut', 'color', 'clarity'])],\n                                   verbose_feature_names_out=False)),\n                ('linearregression', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('standardscaler',\n                                                                   StandardScaler())]),\n                                                  ['x', 'y', 'z', 'carat',\n                                                   'depth', 'table']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse_output=False),\n                                                  ['cut', 'color', 'clarity'])],\n                                   verbose_feature_names_out=False)),\n                ('linearregression', LinearRegression())])columntransformer: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('standardscaler',\n                                                  StandardScaler())]),\n                                 ['x', 'y', 'z', 'carat', 'depth', 'table']),\n                                ('cat',\n                                 OneHotEncoder(handle_unknown='ignore',\n                                               sparse_output=False),\n                                 ['cut', 'color', 'clarity'])],\n                  verbose_feature_names_out=False)num['x', 'y', 'z', 'carat', 'depth', 'table']SimpleImputerSimpleImputer(strategy='median')StandardScalerStandardScaler()cat['cut', 'color', 'clarity']OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse_output=False)LinearRegressionLinearRegression()\n\n\n\n\nCode\nlin_reg[:-1].get_feature_names_out()\n\n\narray(['x', 'y', 'z', 'carat', 'depth', 'table', 'cut_Fair', 'cut_Good',\n       'cut_Ideal', 'cut_Premium', 'cut_Very Good', 'color_D', 'color_E',\n       'color_F', 'color_G', 'color_H', 'color_I', 'color_J',\n       'clarity_I1', 'clarity_IF', 'clarity_SI1', 'clarity_SI2',\n       'clarity_VS1', 'clarity_VS2', 'clarity_VVS1', 'clarity_VVS2'],\n      dtype=object)\n\n\n\n\nCode\nlin_reg_input_features = lin_reg[:-1].get_feature_names_out()\npd.Series(lin_reg[-1].coef_.ravel(), index=lin_reg_input_features).plot.bar()\nplt.tight_layout()\n\n\n\n\n\n\n\nCode\ndiamonds_price_predictor = lin_reg.predict(diamonds_features_test)\n\n\n\n\nCode\nlin_reg[-1].coef_\n\n\narray([-1.25868758e+03,  1.27214638e+02, -2.14508727e+01,  5.34258409e+03,\n       -9.47981200e+01, -6.26319163e+01, -1.32338602e+14, -1.32338602e+14,\n       -1.32338602e+14, -1.32338602e+14, -1.32338602e+14, -1.31891651e+15,\n       -1.31891651e+15, -1.31891651e+15, -1.31891651e+15, -1.31891651e+15,\n       -1.31891651e+15, -1.31891651e+15, -4.95270736e+14, -4.95270736e+14,\n       -4.95270736e+14, -4.95270736e+14, -4.95270736e+14, -4.95270736e+14,\n       -4.95270736e+14, -4.95270736e+14])\n\n\n\n\nCode\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nprint(\"Mean Squared Error: %.2f\" % mean_squared_error(diamonds_price_test, diamonds_price_predictor))\nprint(\"Coefficient of Determination: %.2f\" % r2_score(diamonds_price_test, diamonds_price_predictor))\n\n\nMean Squared Error: 1290106.23\nCoefficient of Determination: 0.92\n\n\n\n\nCode\nfrom sklearn.linear_model import SGDRegressor\n\nsgd_reg = make_pipeline(preprocessor, SGDRegressor(max_iter=1000, tol=1e-5, penalty=None, eta0=0.01, n_iter_no_change=100, random_state=42))\nsgd_reg.fit(diamonds_features_train, diamonds_price_train)\n\n\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('standardscaler',\n                                                                   StandardScaler())]),\n                                                  ['x', 'y', 'z', 'carat',\n                                                   'depth', 'table']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse_output=False),\n                                                  ['cut', 'color', 'clarity'])],\n                                   verbose_feature_names_out=False)),\n                ('sgdregressor',\n                 SGDRegressor(n_iter_no_change=100, penalty=None,\n                              random_state=42, tol=1e-05))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('standardscaler',\n                                                                   StandardScaler())]),\n                                                  ['x', 'y', 'z', 'carat',\n                                                   'depth', 'table']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse_output=False),\n                                                  ['cut', 'color', 'clarity'])],\n                                   verbose_feature_names_out=False)),\n                ('sgdregressor',\n                 SGDRegressor(n_iter_no_change=100, penalty=None,\n                              random_state=42, tol=1e-05))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('standardscaler',\n                                                  StandardScaler())]),\n                                 ['x', 'y', 'z', 'carat', 'depth', 'table']),\n                                ('cat',\n                                 OneHotEncoder(handle_unknown='ignore',\n                                               sparse_output=False),\n                                 ['cut', 'color', 'clarity'])],\n                  verbose_feature_names_out=False)num['x', 'y', 'z', 'carat', 'depth', 'table']SimpleImputerSimpleImputer(strategy='median')StandardScalerStandardScaler()cat['cut', 'color', 'clarity']OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse_output=False)SGDRegressorSGDRegressor(n_iter_no_change=100, penalty=None, random_state=42, tol=1e-05)\n\n\n\n\nCode\nprice_predictor_sgd = sgd_reg.predict(diamonds_features_test)\n\n\n\n\nCode\nprint(\"Mean Squared Error: %.2f\" % mean_squared_error(diamonds_price_test, price_predictor_sgd))\nprint(\"Coefficient of Determination: %.2f\" % r2_score(diamonds_price_test, price_predictor_sgd))\n\n\nMean Squared Error: 1288354.76\nCoefficient of Determination: 0.92\n\n\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\n\nrfr_reg = make_pipeline(preprocessor, RandomForestRegressor())\nrfr_reg.fit(diamonds_features_train, diamonds_price_train)\n\n\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('standardscaler',\n                                                                   StandardScaler())]),\n                                                  ['x', 'y', 'z', 'carat',\n                                                   'depth', 'table']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse_output=False),\n                                                  ['cut', 'color', 'clarity'])],\n                                   verbose_feature_names_out=False)),\n                ('randomforestregressor', RandomForestRegressor())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('standardscaler',\n                                                                   StandardScaler())]),\n                                                  ['x', 'y', 'z', 'carat',\n                                                   'depth', 'table']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse_output=False),\n                                                  ['cut', 'color', 'clarity'])],\n                                   verbose_feature_names_out=False)),\n                ('randomforestregressor', RandomForestRegressor())])columntransformer: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('standardscaler',\n                                                  StandardScaler())]),\n                                 ['x', 'y', 'z', 'carat', 'depth', 'table']),\n                                ('cat',\n                                 OneHotEncoder(handle_unknown='ignore',\n                                               sparse_output=False),\n                                 ['cut', 'color', 'clarity'])],\n                  verbose_feature_names_out=False)num['x', 'y', 'z', 'carat', 'depth', 'table']SimpleImputerSimpleImputer(strategy='median')StandardScalerStandardScaler()cat['cut', 'color', 'clarity']OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse_output=False)RandomForestRegressorRandomForestRegressor()\n\n\n\n\nCode\nprice_predictor_rfr = rfr_reg.predict(diamonds_features_test)\n\n\n\n\nCode\nprint(\"Mean Squared Error: %.2f\" % mean_squared_error(diamonds_price_test, price_predictor_rfr))\nprint(\"Coefficient of Determination: %.2f\" % r2_score(diamonds_price_test, price_predictor_rfr))\n\n\nMean Squared Error: 301225.87\nCoefficient of Determination: 0.98\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Unsupervised Learning: Clustering Using K-Means and Gaussian Mixture Models",
    "section": "",
    "text": "So far we have seen regression and classification algorithms and their many variants. In this blog, we will be exploring another type of learning called Unsupervised Learning. Unlike in regression and classification where we have response variable or target classes, we will not have any predefined labels in unsupervised learning. It is up for the algorithm to learn the similarities and differences and then group the instances that belong together. We will be working with the Penguins dataset in this blog to perform unsupervised learning. At a high level, we will -"
  },
  {
    "objectID": "posts/Clustering/index.html#import-and-analyze-penguins-dataset",
    "href": "posts/Clustering/index.html#import-and-analyze-penguins-dataset",
    "title": "Unsupervised Learning: Clustering Using K-Means and Gaussian Mixture Models",
    "section": "1. Import and Analyze Penguins Dataset",
    "text": "1. Import and Analyze Penguins Dataset\nThe penguins dataset is available in the seaborn visualization package. Let’s load the dataset and analyze its features and instances.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npenguins = sns.load_dataset('penguins')\nprint(f\"Number of Features: {penguins.shape[0]}\")\nprint(f\"Number of Observations: {penguins.shape[1]}\")\n\nprint(f\"Features - {list(penguins.columns)}\")\n\nNumber of Features: 344\nNumber of Observations: 7\nFeatures - ['species', 'island', 'bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex']\n\n\nLet us see unique species, islands from the penguins dataset and also the range of values of the numerical features take using pandas describe method. Let us also ensure the dataset is free from any nulls or missing values (NaN) by calculating the percentage of Nulls or NaNs in each column.\n\n# ['species', 'island', 'bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex']\nprint(f\"Unique colors: {penguins['species'].unique()}\")\nprint(f\"Unique cut type: {penguins['island'].unique()}\\n\")\n\nprint(penguins.describe(), '\\n')\n\npercent_missing = ((penguins.isnull().sum() + penguins.isna().sum()) / len(penguins)) * 100\nmissing_values_percent_df = percent_missing.to_frame('Missing Data Percent')\nprint(missing_values_percent_df.round(2))\n\nUnique colors: ['Adelie' 'Chinstrap' 'Gentoo']\nUnique cut type: ['Torgersen' 'Biscoe' 'Dream']\n\n       bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\ncount      342.000000     342.000000         342.000000   342.000000\nmean        43.921930      17.151170         200.915205  4201.754386\nstd          5.459584       1.974793          14.061714   801.954536\nmin         32.100000      13.100000         172.000000  2700.000000\n25%         39.225000      15.600000         190.000000  3550.000000\n50%         44.450000      17.300000         197.000000  4050.000000\n75%         48.500000      18.700000         213.000000  4750.000000\nmax         59.600000      21.500000         231.000000  6300.000000 \n\n                   Missing Data Percent\nspecies                            0.00\nisland                             0.00\nbill_length_mm                     1.16\nbill_depth_mm                      1.16\nflipper_length_mm                  1.16\nbody_mass_g                        1.16\nsex                                6.40\n\n\nWe do have missing or null values in five of the columns. Because this is a very small percentage, lets us drop these records.\n\npenguins.dropna(inplace=True)\nprint(f\"Dataset Shaper After Removing Nulls: {penguins.shape}\\n\")\npercent_missing = ((penguins.isnull().sum() + penguins.isna().sum()) / len(penguins)) * 100\nmissing_values_percent_df = percent_missing.to_frame('Missing Data Percent')\nprint(missing_values_percent_df.round(2))\n\nDataset Shaper After Removing Nulls: (333, 7)\n\n                   Missing Data Percent\nspecies                             0.0\nisland                              0.0\nbill_length_mm                      0.0\nbill_depth_mm                       0.0\nflipper_length_mm                   0.0\nbody_mass_g                         0.0\nsex                                 0.0\n\n\n!Perfect. We now have cleaned the dataset from Nulls and NaNs.\nLet us now visualize the features bill_length, bill_depth, and flipper_length using a scatterplot. For this you can use the pandas inbuilt scattermatrix plot. But using Seaborn helps us to add hue, using species labels. This helps us better differentiate the underlying clusters in our dataset.\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 6))\naxs[0] = sns.scatterplot(penguins, x='bill_length_mm', y='bill_depth_mm', hue='species', ax=axs[0])\naxs[1] = sns.scatterplot(penguins, x='flipper_length_mm', y='bill_length_mm', hue='species', ax=axs[1])\nplt.show()\n\n\n\n\nLooks like our data does have an underlying clusters and we seem to have three clusters one for each species type - Adelie, Gentoo, and Chinstrap. We can now apply K-Means clustering."
  },
  {
    "objectID": "posts/Clustering/index.html#apply-k-means-clustering",
    "href": "posts/Clustering/index.html#apply-k-means-clustering",
    "title": "Unsupervised Learning: Clustering Using K-Means and Gaussian Mixture Models",
    "section": "2. Apply K-Means Clustering",
    "text": "2. Apply K-Means Clustering\nK-means is an unsupervised learning algorithm that is capable of clustering a given dataset of instances quickly and efficiently into K clusters. It works by assigning K random instances as centroids to K clusters and then assign remaining instances as the members to a cluster that it is nearest to. It calculates the distance using the mean squared distance between the instances and their closest centroids. It then calculates the centroids using the newly formed clusters. And then the instances are re-assigned after computing the distance to newly formed clusters. This process continues until there is no further shift in centroids. To put it formally, the objective at each step is to minimize the following within-cluster sum of squares distance-\n\\[ {\\displaystyle \\mathop {\\operatorname {arg\\,min} } _{\\mathbf {S} }\\sum _{i=1}^{k}\\sum _{\\mathbf {x} \\in S_{i}}\\left\\|\\mathbf {x} -{\\boldsymbol {\\mu }}_{i}\\right\\|^{2}=\\mathop {\\operatorname {arg\\,min} } _{\\mathbf {S} }\\sum _{i=1}^{k}|S_{i}|\\operatorname {Var} S_{i}} \\]\nwhere, \\(x_i\\) denote the n observations in the dataset \\(x_1, x_2, x_3, ..., x_n\\) \\(S_i = {S_1, S_2, S_3, ..., S_k}\\) denote the k sets/clusters \\(\\mu_i\\) is the mean or centroid of the points in \\(S_i\\)\nBefore we start applying K-means on penguins dataset, a couple of things to take care of. We need to scale our features. Clustering algorithms are sensitive to scale and Scaling ensures that all features in the data are weighted equally. This is very important because clustering algorithms use distance between data points to assess the similarity between them. A simple way to do this is transforming the features into their Z-scores.\n\\[ z={x-\\mu  \\over \\sigma } \\]\nWe can simply pass the features to scikit-learns StandardScalar to do the same.\n\nfrom sklearn.preprocessing import StandardScaler\npenguins_numerical_features = penguins.select_dtypes(include=np.number)\nscaler = StandardScaler()\npenguins_scaled = scaler.fit_transform(penguins_numerical_features)\npenguins_scaled_df = pd.DataFrame(penguins_scaled, columns=penguins_numerical_features.columns, index=None)\n\nGreat! our features are scaled. Now lets feed them to the K-Means algorithm to see if it can figure out the clusters in our dataset that we have found earlier. Let us set the initial number of clusters to be 3 because we know there to be three different species in our dataset. Initializing with correct number of clusters is not always possible, but in this case we have the ground truth, so we can use that to initialize k.\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nimport matplotlib as mpl\nn_clusters = 3\nk = 3\n\npca = PCA(n_components=3, svd_solver='full')\npenguins_reduced = pca.fit_transform(penguins_scaled_df)\n# print(\"Cumulative Explained Variance (%): \", np.cumsum(pca.explained_variance_ratio_ * 100).round(2))\n\nkmeans = KMeans(init=\"k-means++\", n_clusters=3, n_init=4)\nkmeans.fit(penguins_reduced)\nk_means_cluster_centers = kmeans.cluster_centers_\n\nprint(f\"The Cluster Centers are - \\n{k_means_cluster_centers}\")\n\n# PC1 & PC2\npd.DataFrame(\n    penguins_reduced, \n    columns=['PC1', 'PC2', 'PC3']\n).plot.scatter(\n    x=0, y=1, c=kmeans.labels_, cmap='viridis', colorbar=False\n)\nplt.scatter(x=-0.39, y=1.103, s=100, c='r', marker='o')\nplt.scatter(x=-0.39, y=1.103, s=100, c='w', marker='x')\nplt.scatter(x=2.01, y=-0.394, s=100, c='r', marker='o')\nplt.scatter(x=2.01, y=-0.394, s=100, c='w', marker='x')\nplt.scatter(x=-1.5, y=-0.3, s=100, c='r', marker='o')\nplt.scatter(x=-1.5, y=-0.3, s=100, c='w', marker='x')\nplt.title(\"Scatter plot of Reduced Data - PCA1 vs PCA2\")\n\n\n# PC1 & PC3\npd.DataFrame(\n    penguins_reduced, \n    columns=['PC1', 'PC2', 'PC3']\n).plot.scatter(\n    x=0, y=2, c=kmeans.labels_, cmap='viridis', colorbar=False\n)\nplt.scatter(x=-0.39, y=0.384, s=100, c='r', marker='o')\nplt.scatter(x=-0.39, y=0.384, s=100, c='w', marker='x')\nplt.scatter(x=2.01, y=-0.035, s=100, c='r', marker='o')\nplt.scatter(x=2.01, y=-0.035, s=100, c='w', marker='x')\nplt.scatter(x=-1.5, y=-0.22, s=100, c='r', marker='o')\nplt.scatter(x=-1.5, y=-0.22, s=100, c='w', marker='x')\nplt.title(\"Scatter plot of Reduced Data - PCA1 vs PCA3\")\n\nThe Cluster Centers are - \n[[-1.50185976 -0.21065706 -0.26162702]\n [ 2.01297608 -0.39402785 -0.03586763]\n [-0.33251925  1.103199    0.59917913]]\n\n\nText(0.5, 1.0, 'Scatter plot of Reduced Data - PCA1 vs PCA3')\n\n\n\n\n\n\n\n\nThe k-means algorithm was able to create three clusters and assign the instances to these three clusters. As we can see from the above plot, the clusters on the left have some instances overlapping and there is no clear decision boundary. Let us now evaluate this model’s performance."
  },
  {
    "objectID": "posts/Clustering/index.html#evaluating-k-means-performance-and-choosing-optimal-number-of-clusters",
    "href": "posts/Clustering/index.html#evaluating-k-means-performance-and-choosing-optimal-number-of-clusters",
    "title": "Unsupervised Learning: Clustering Using K-Means and Gaussian Mixture Models",
    "section": "Evaluating K-Means Performance and Choosing Optimal Number of Clusters",
    "text": "Evaluating K-Means Performance and Choosing Optimal Number of Clusters\n\n3.1 Inertia & Elbow Curve\nOne way to initialize the number of clusters is to run a clustering algorithm and know what could be the approximate number of clusters. Anathor approach is to run the algorithm multiple times using different values for k, in scikit-learn KMeans this is controlled by n_init hyperparameter. At each random initialization some performance metric is calculated and the model with best performance is retained. For K-Means this metric is the model’s inertia. It is calculated by summing the squared distances between each data point and its closest centroid. As you would guess, lower the inertia of the model, better the clustering. Scikit-learn also provides score() method, which is negative of the inertia. Higher the score value (closer to zero) better the performance.\n\nprint(\"Inertia: \", kmeans.inertia_)\nprint(\"Score: \", kmeans.score(penguins_reduced))\n\nInertia:  336.133975360404\nScore:  -336.13397536040395\n\n\nThe scores does not look so good even with a good initialization of n_clusters=3. This has to do with Clusters of varying sizes and density. k-means has trouble clustering data where clusters are of varying sizes and density. To see the inertia of the model initialized with different values of k, we can make use of the elbow curve as shown below. As we can see, the kmeans algorithm gives better performance with k greater than 3 (lower inertia). Which as we know is not correct. So inertia is not always perfect measure of the models performance.\n\nkmeans_per_k = [KMeans(n_clusters=k, n_init=10, random_state=42).fit(penguins_reduced)\n                for k in range(1, 10)]\ninertias = [model.inertia_ for model in kmeans_per_k]\n\nplt.figure(figsize=(8, 3.5))\nplt.plot(range(1, 10), inertias, \"bo-\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Inertia\")\nplt.axis([1, 8.5, 0, 1500])\nplt.grid('both')\nplt.show()\n\n\n\n\n\n\n3.2 Silhouette Score\nA more precise but computationally expensive metric is the model’s Silhouette Score which is calculated as the mean silhouette coefficient over all the instances. An instance’s silhouette coefficient is calculated as \\((b-a)/max(a, b)\\), where a is the mean distances to other instances in the same cluster and b is the mean distance to the instances of the next closest cluster. The silhouette coefficient can vary between –1 and +1. A coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary; finally, a coefficient close to –1 means that the instance may have been assigned to the wrong cluster.\n\nfrom sklearn.metrics import silhouette_score\nsilhouette_score(penguins_reduced, kmeans.labels_)\n\n0.4840867203220288\n\n\n\nsilhouette_scores = [silhouette_score(penguins_reduced, model.labels_)\n                     for model in kmeans_per_k[1:]]\n\nplt.figure(figsize=(8, 3))\nplt.plot(range(2, 10), silhouette_scores, \"bo-\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Silhouette score\")\n# plt.axis([1.8, 8.5, 0.55, 0.7])\nplt.grid('both')\nplt.show()\n\n\n\n\nOnce again, we can see that for k greater than 3, the silhoutte score is getting closer to 0. Which suggests more instances are close to the cluster boundary. We need a better methodology to cluster our dataset."
  },
  {
    "objectID": "posts/Clustering/index.html#gaussian-mixture-models",
    "href": "posts/Clustering/index.html#gaussian-mixture-models",
    "title": "Unsupervised Learning: Clustering Using K-Means and Gaussian Mixture Models",
    "section": "4. Gaussian Mixture Models",
    "text": "4. Gaussian Mixture Models\nThe Gaussian Mixture Model is a probabilistic model that assumes that instances are drawn from multiple but finite number of Gaussian (normal) distributions whose parameters are unknown. Typically, all the instances generated from a single Gaussian distribution form an elliptical shaped cluster. There are several variations to Gaussian Mixture Models. The one we will implement using scikit-learn is the GaussianMixture which requires us to know the number of clusters (k) in advance. Usually, it is a good idea to run kmeans or mini-batch kmeans to help with figuring the optimum value for k. Given a dataset X, the GaussianMixture works as follows -\n\nFor each instance in X, a cluster is randomly picked among k clusters. The probability of choosing a cluster \\(j\\) is given by clusters weight \\(\\theta(j)\\) and the index of the cluster choosen for instance i is noted by \\(Z(i)\\).\nIf instance i is assigned to cluster j i.e., \\(Z(i) = j\\), then the location of this instance is sampled randomly from the Gaussian Distribution with \\(\\mu(j)\\) and covariance matrix \\(\\Sigma(j)\\). This is noted by \\(x(i) \\sim \\Nu(\\mu(j), \\Sigma(j))\\)\n\nGMMs are particularly useful when the data is not clearly separable into distinct clusters. We may be able to solve our problem with the cluster overlapping that we have seen with k-means clustering. Let us apply the GaussianMixture algorithm to our Penguins dataset. Let us set n_components and run the algorithm for 10 iterations.\n\nfrom sklearn.mixture import GaussianMixture\n\ngm = GaussianMixture(n_components=3, n_init=10)\ny_labels = gm.fit_predict(penguins_reduced)\n\nWe can ectract the weights of the clusters formed by the GaussianMixtures algorithm and compare it with the weights of the raw dataset.\n\nmodel_weights = (gm.weights_ * 100).round(2)\nraw_weights = ((penguins['species'].value_counts() / len(penguins)) * 100).round(2)\n\nprint(f\"Weights of Clusters formed by GMM: \\n\\t\\t {model_weights}\\n\")\nprint(f\"Weights of Clusters from Raw Data: \\n {raw_weights}\")\n\nWeights of Clusters formed by GMM: \n         [35.74 44.32 19.94]\n\nWeights of Clusters from Raw Data: \n species\nAdelie       43.84\nGentoo       35.74\nChinstrap    20.42\nName: count, dtype: float64\n\n\nGreat! Our model is able to predict three clusters and the weights also look pretty close to percentage of different species we have in our dataset. Furthermore, the cluster centroids and the corresponding covariance matrices can be extracted using means_ and covariances_ parameters.\n\nprint(f\"Cluster Centroids: \\n {(gm.means_).round(3)}\\n\")\nprint(f\"Covariance Matrix for each corresponding distribution: \\n {(gm.covariances_).round(3)}\")\n\nCluster Centroids: \n [[ 2.013 -0.394 -0.036]\n [-1.468 -0.15  -0.292]\n [-0.344  1.039  0.714]]\n\nCovariance Matrix for each corresponding distribution: \n [[[ 0.394  0.372 -0.159]\n  [ 0.372  0.504 -0.176]\n  [-0.159 -0.176  0.196]]\n\n [[ 0.28   0.173 -0.087]\n  [ 0.173  0.472 -0.203]\n  [-0.087 -0.203  0.265]]\n\n [[ 0.267  0.241 -0.1  ]\n  [ 0.241  0.54  -0.096]\n  [-0.1   -0.096  0.208]]]\n\n\n\nplt.figure()\nplt.scatter(penguins_reduced[:, 0], penguins_reduced[:, 2], c=y_labels)    \nfor i in range(3):    \n    plt.scatter(x=gm.means_[i][0], y=gm.means_[i][2], s=100, c='b', marker='o')\n    plt.scatter(x=gm.means_[i][0], y=gm.means_[i][2], s=100, c='w', marker='x')\nplt.title(\"Penguins Dataset Clustered using Gaussian Mixture Model\")\nplt.xlabel(r'$x_1$')\nplt.ylabel(r'$x_3$')\nplt.show()"
  },
  {
    "objectID": "posts/Clustering/index.html#conclusion",
    "href": "posts/Clustering/index.html#conclusion",
    "title": "Unsupervised Learning: Clustering Using K-Means and Gaussian Mixture Models",
    "section": "Conclusion",
    "text": "Conclusion\nUnsupervised learning is an amazing and useful technique that can be used to discover hidden patterns and structures in data without the need for labeled examples. We have learned about Clustering and the most widely used and easy to implement K-Means Clustering. We learned that K-Means has some limitations and it is usually a good idea to start with k-means of one of its variants before trying out advanced clustering algorithms like DBSCAN or Gaussian Mixture Models. We have also seen how Gaussian Mixtures work and how to use them to train on data that has clusters closely packed and inseparable by other algorithms."
  },
  {
    "objectID": "posts/Anomaly Detection/index.html",
    "href": "posts/Anomaly Detection/index.html",
    "title": "Anomaly Detection and Synthetic Data Generation with Scikit-Learn",
    "section": "",
    "text": "Until now, we have worked with real world datasets and applied several Machine Learning algorithms to them. Sometimes it is also useful to generate synthetic data that closely simulates some real-world examples. In this blogpost, we will see how to generate synthetic data with scikit-learn. Specifically, we will create clusters of data with some anomalies which helps us to learn and apply some Anomaly Detection algorithms. At a high level, we will -"
  },
  {
    "objectID": "posts/Anomaly Detection/index.html#generating-synthetic-data-with-scikit-learn",
    "href": "posts/Anomaly Detection/index.html#generating-synthetic-data-with-scikit-learn",
    "title": "Anomaly Detection and Synthetic Data Generation with Scikit-Learn",
    "section": "1. Generating Synthetic Data with Scikit-learn",
    "text": "1. Generating Synthetic Data with Scikit-learn\nScikit-learn has built-in synthetic data generators that can be used for a variety of machine learning tasks. Two such generators are make_blobs and make_classification which can create multiclass datasets containing normally distributed clusters of points for each class. In this blog post, we will use the make_blobs generator as it allows greater control over the placement of cluster centers and the standard deviation of points within each cluster. Let us start be creating few clusters using make_blobs.\n\nimport numpy as np\nfrom sklearn.datasets import make_blobs\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\nX, y, centers = make_blobs(n_samples=1500, \n                           centers=4, \n                           n_features=2, \n                           cluster_std=1.8, \n                           random_state=5805, \n                           return_centers=True, \n                           shuffle=True)\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\ncenters_scaled = scaler.transform(centers)\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap='tab10')\nplt.title(\"Four Randomly Generated Gaussian Spherical Blobs with $\\sigma=1.8$\")\nfor i in range(4):\n    plt.scatter(x=centers_scaled[i][0], y=centers_scaled[i][1], s=100, c='black', marker='o')\n    plt.scatter(x=centers_scaled[i][0], y=centers_scaled[i][1], s=100, c='white', marker='x')\nplt.show()\n\nprint(f\"The Centers of the clusters are at: \\n{centers_scaled}\")\n\n\n\n\nThe Centers of the clusters are at: \n[[-0.42055743 -1.02171133]\n [ 1.52980404 -0.75580787]\n [-0.80809373  0.3622626 ]\n [-0.29674727  1.43940821]]\n\n\nThe above code generated four clusters with the specified standard deviation. Internally make_blobs uses a generative model called the Gaussian Mixture Model(GMM), which is a statistical model that assumes instances (data points) to belong to a finite mixture of Gaussian (normal) distributions with some known parameters. Simply put, GMM is a way of representing dataset as a collection of Gaussian distributions. Each of these distributions represent a cluster of data points, just like the ones shown above. Later in this blog, we will see how to use GMM to detect outliers/anomalies in our data.\n\nLinear Transformations\nAnother useful trick when generating synthetic data is applying linear transformations. Sometimes, we want to make the clusters elliptical rather than circular, or bring the clusters closer to evaluate the performance of the algorithms we want to use. Linear transformations are a great way to do this. Below are the list of transformations and their corresponding matrix forms that are most commonly used -\n\nMatrix for Horizontal shear \\[ \\huge A=\\begin{bmatrix}\n1 &k \\\\\n0 &1\n\\end{bmatrix} \\]\nMatrix for Vertical shear \\[ \\huge A=\\begin{bmatrix}\n1 &0 \\\\\nk &1\n\\end{bmatrix} \\]\nMatrix for Counterclockwise Rotation \\[ \\huge A=\\begin{bmatrix}\n\\cos(\\theta) &-\\sin(\\theta) \\\\\n\\sin(\\theta) &\\cos(\\theta)\n\\end{bmatrix} \\]\nMatrix for reflection about horizontal axis \\[ \\huge A=\\begin{bmatrix}\n1 &0 \\\\\n0 &-1\n\\end{bmatrix} \\]\n\nLet us apply these transformations on our synthetic data to see how the transformed data look.\n\nfig = plt.figure(figsize=(10, 10))\nax1 = fig.add_subplot(221)\n\n# Rotation (Clockwise)\nangle45 = 45 * np.pi/180\nangle135 = 135 * np.pi/180\nrotation_transformation = np.array([\n    [np.cos(angle45), np.sin(angle45)], \n    [np.cos(angle135), np.sin(angle135)]\n])\n\nX_rotated = X.dot(rotation_transformation)\nax1.scatter(X_rotated[:, 0], X_rotated[:, 1], c=y, cmap='tab10')\nax1.set_title(\"Clockwise Rotation\")\n\n\n# reflection - Horizontal Axis\nax2 = fig.add_subplot(222)\nreflection_matrix = np.array([\n    [1, 0], \n    [0, -1]\n])\nX_transformed = X.dot(reflection_matrix)\nax2.scatter(X_transformed[:, 0], X_transformed[:, 1], c=y, cmap='tab10')\nax2.set_title(\"Rotation about Horizontal Axis\")\n\n# Horizontal Shearing\nax3 = fig.add_subplot(223)\nhshear_matrix = np.array([\n    [1, 1.5], \n    [0, 1]\n])\nX_hsheared = X.dot(hshear_matrix)\nax3.scatter(X_hsheared[:, 0], X_hsheared[:, 1], c=y, cmap='tab10')\nax3.set_title(\"Horizontal Shearing\")\n\n# Vertical Shearing\nax4 = fig.add_subplot(224)\nvshear_matrix = np.array([\n    [1, 0], \n    [1.5, 1]\n])\nX_vsheared = X.dot(vshear_matrix)\nax4.scatter(X_vsheared[:, 0], X_vsheared[:, 1], c=y, cmap='tab10')\nax4.set_title(\"Vertical Shearing\")\nplt.show()\n\n\n\n\nYou can pick and choose the transformation according to the type of problem you are simulating or the algorithm you are working with. Now that we have our data ready, let us go on a hunt for the outliers!"
  },
  {
    "objectID": "posts/Anomaly Detection/index.html#applying-dbscan-to-predict-outliersanomalies",
    "href": "posts/Anomaly Detection/index.html#applying-dbscan-to-predict-outliersanomalies",
    "title": "Anomaly Detection and Synthetic Data Generation with Scikit-Learn",
    "section": "2. Applying DBSCAN to Predict Outliers/Anomalies",
    "text": "2. Applying DBSCAN to Predict Outliers/Anomalies\nThe Density Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm defines clusters as continuous regions of high density. The algorithm works by defining a neighborhood around each point in the dataset. A neighborhood is defined by two parameters: \\(\\epsilon\\) (epsilon) and min_samples. \\(\\epsilon\\) specifies a small distance (radius) around each instance and min_samples denote the minimum number of instances required to be in that instances \\(\\epsilon\\)-neighborhood for it to be considered a core instance. All instances in the neighborhood of a core instance belong to the same cluster and any instance that is not a core instance and does not have one in its neighborhood is considered an Anomaly.\n\nfrom sklearn.cluster import DBSCAN\n\ndbscan = DBSCAN(eps=0.2, min_samples=10)\ndbscan.fit(X)\n\nDBSCAN(eps=0.2, min_samples=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DBSCANDBSCAN(eps=0.2, min_samples=10)\n\n\n\nlabels = dbscan.labels_\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nunique_labels = set(labels)\ncore_samples_mask = np.zeros_like(labels, dtype=bool)\ncore_samples_mask[dbscan.core_sample_indices_] = True\n\ncolors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = labels == k\n\n    xy = X[class_member_mask & core_samples_mask]\n    plt.plot(\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=14,\n    )\n\n    xy = X[class_member_mask & ~core_samples_mask]\n    plt.plot(\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=6,\n    )\n\nplt.title(f\"Estimated number of clusters: {n_clusters_}\")\nplt.show()\n\nfrom sklearn.metrics import silhouette_score, homogeneity_score\nprint(f\"silhouette_score: {silhouette_score(X, labels):.2f}\")\nprint(f\"homogeneity_score: {homogeneity_score(y, labels):.2f}\")\n\n\n\n\nsilhouette_score: 0.54\nhomogeneity_score: 0.70\n\n\nLooks like the DBSCAN algorithm could not make out all four clusters in our dataset. But yet it was able to spot many outliers. Let us validate our results with a more precise algorithm like Gaussian Mixture to help cluster the dataset accurately while also predict the outliers."
  },
  {
    "objectID": "posts/Anomaly Detection/index.html#gaussian-mixture-models-for-anomaly-detection",
    "href": "posts/Anomaly Detection/index.html#gaussian-mixture-models-for-anomaly-detection",
    "title": "Anomaly Detection and Synthetic Data Generation with Scikit-Learn",
    "section": "3. Gaussian Mixture Models for Anomaly Detection",
    "text": "3. Gaussian Mixture Models for Anomaly Detection\nWe have seen how to use Gaussian Mixture Models to perform clustering tasks when data points are of varying sizes and densities. Another application of GMMs are in Anomaly and Novelty detection. Because they make use of normal distributions and probability densities. We can identify the points with low probability density and label them as anomalies. However, here again choosing the right number of gaussian clusters for the GMM model is essential, incorrectly choosing the number of components can cause the model to either under-fit or over-fit.\n\nfrom sklearn.mixture import GaussianMixture\n\ngm = GaussianMixture(n_components=4, n_init=10)\ngm.fit(X)\nprint(f\"cluster centers predicted by Gaussian Mixture Model: \\n {gm.means_}\\n\")\nprint(f\"Actual Centers: \\n {centers_scaled}\") \n\ncluster centers predicted by Gaussian Mixture Model: \n [[-0.28867979  1.42304583]\n [-0.39582404 -1.00586282]\n [ 1.53680021 -0.76325421]\n [-0.82158344  0.35570973]]\n\nActual Centers: \n [[-0.42055743 -1.02171133]\n [ 1.52980404 -0.75580787]\n [-0.80809373  0.3622626 ]\n [-0.29674727  1.43940821]]\n\n\nAs you can see the centers predicted by the GaussianMixture model is pretty close to the actual centers from or generated data. Also note that unlike DBSCAN, the GaussianMixture was able to correctly predict four clusters in our dataset. Let us label and plot these anomalies.\n\nimport seaborn as sns\ndensities = gm.score_samples(X)\ndensity_threshold = np.percentile(densities, 2)\ncluster_points = X[densities &gt;= density_threshold]\nanomalies = X[densities &lt; density_threshold]\n\ny_labels = gm.fit_predict(cluster_points)\n\nplt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=y_labels, cmap='tab10')\nplt.scatter(anomalies[:, 0], anomalies[:, 1], c='black', marker='*')\nplt.title(\"Anomalies Detected by GaussianMixture Model\")\nplt.show()"
  },
  {
    "objectID": "posts/Anomaly Detection/index.html#conclusion-closing-remarks",
    "href": "posts/Anomaly Detection/index.html#conclusion-closing-remarks",
    "title": "Anomaly Detection and Synthetic Data Generation with Scikit-Learn",
    "section": "Conclusion & Closing Remarks",
    "text": "Conclusion & Closing Remarks\nDBSCAN and Gaussian Mixture Models are powerful unsupervised learning algorithms for clustering and anomaly detection tasks. Each has its own strengths and weaknesses, and choosing the right one depends on the data and problem being tackled. GMMs are particularly good at dealing with complex data distributions, over-lapping and non-spherical clusters. They can also be used for soft clustering to assign multiple probabilities to each point. DBSCAN, on the other hand, is less affected by data noise and can identify clusters with arbitrary shapes. It is, therefore, important to choose the best algorithm that suits the data and problem at hand.\nWe also saw how to generate synthetic data using scikit_learn in this blog post. Working with such datasets is both cost-effective and hugely beneficial when specific needs or conditions are not met by the real world data or when privacy concerns limit the data availability. Synthetic datasets are scalable and can help simulate ‘what if’ scenarios, test a hypothesis or model multiple outcomes."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sai Sundeep Rayidi",
    "section": "",
    "text": "Hi, I am Sai, a Computer Science graduate student at Virginia Tech. I blog about Data Analytics, Visualization and Machine Learning. When not innovating on these topics, I enjoy reading, running, cooking, and playing cricket."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Sai Sundeep Rayidi",
    "section": "Education",
    "text": "Education\nVirginia Tech, Falls Church | D.C. Area, VA MEng in Computer Science and Application | August 2023 - Present\nJawaharlal Nehru Technological University | Hyderabad, India Bachelors in Computer Science and Engineering | August 2015 - May 2019"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Sai Sundeep Rayidi",
    "section": "Experience",
    "text": "Experience\nVerizon AI & DATA | Data Engineer | October 2021 - June 2023 Verizon Global Network & Technology | Systems Engineer | August 2019 - September 2021"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Series",
    "section": "",
    "text": "Multiclass Classification on Fashion MNIST Dataset\n\n\n\n\n\n\n\nclassification\n\n\nPCA analysis\n\n\nClassification Performance\n\n\nRandomizedSearch\n\n\nMulti-Class\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2023\n\n\nSai Sundeep Rayidi\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory - Gaussian Distribution, Normality Tests, and Z-Scores\n\n\n\n\n\n\n\nProbability Theory\n\n\nGaussian Distribution\n\n\nVisualization\n\n\nNormality Test\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2023\n\n\nSai Sundeep Rayidi\n\n\n\n\n\n\n  \n\n\n\n\nUnsupervised Learning: Clustering Using K-Means and Gaussian Mixture Models\n\n\n\n\n\n\n\nclustering\n\n\nanalysis\n\n\nvisualization\n\n\nK-Means\n\n\nGaussian Mixtures\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nSai Sundeep Rayidi\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly Detection and Synthetic Data Generation with Scikit-Learn\n\n\n\n\n\n\n\nAnomaly Detection\n\n\nDBSCAN\n\n\nSynthetic Data\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nSai Sundeep Rayidi\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Non-Linear Regression\n\n\n\n\n\n\n\nLinear Regression\n\n\nNon-Linear Regression\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nSai Sundeep Rayidi\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Multiclass Classification on Fashion MNIST Dataset",
    "section": "",
    "text": "In this blog we will be performing multiclass classification on the Fashion MNIST dataset. The dataset contains 60,000 images for training and 10,000 images for validation of 10 different types of fashion products. Multiclass classification is a classification task with more than two classes. Each sample can only be labeled as one class. For example, in the context of fashion-mnist images, each image can be of either a shirt, a sneaker, or a trouser. Each image is one sample and is labeled as one of the 10 possible classes. Multiclass classification makes the assumption that each sample is assigned to one and only one label - one sample cannot, for example, be both a shirt and a coat. We will be performing the following tasks in this blog -\n\nImport, analyze, and visualize the fashion-mnist dataset.\nPrepare the data for classification\n\nPrincipal Component Analysis\nHyperparameter Tuning with Randomized Search\n\nTrain a RandomForestClassifier and visualize its performance metrics\nObservations and Conclusion\n\nLet us begin by importing a few packages and loading the data into a pandas dataframe.\n\n1. Import, analyze, and visualize the fashion-mnist dataset.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(category=UserWarning, action='ignore')\n\n\n# Load the train and test datasets\nfmnist_train_df = pd.read_csv(\"fashion_mnist_train.csv\")\nfmnist_test_df = pd.read_csv(\"fashion_mnist_test.csv\")\n\n\nprint(f'Train Set Dimensions - {fmnist_train_df.shape}')\nprint(f'Test Set Dimensions - {fmnist_test_df.shape}')\n\nTrain Set Dimensions - (60000, 785)\nTest Set Dimensions - (10000, 785)\n\n\nLet us print a few records from the training set and see how the data looks like.\n\nfmnist_train_df.head(10)\n\n\n\n\n\n\n\n\nlabel\npixel1\npixel2\npixel3\npixel4\npixel5\npixel6\npixel7\npixel8\npixel9\n...\npixel775\npixel776\npixel777\npixel778\npixel779\npixel780\npixel781\npixel782\npixel783\npixel784\n\n\n\n\n0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n9\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n6\n0\n0\n0\n0\n0\n0\n0\n5\n0\n...\n0\n0\n0\n30\n43\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n1\n2\n0\n0\n0\n0\n...\n3\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n4\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n5\n4\n0\n0\n0\n5\n4\n5\n5\n3\n5\n...\n7\n8\n7\n4\n3\n7\n5\n0\n0\n0\n\n\n6\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n14\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n7\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n8\n4\n0\n0\n0\n0\n0\n0\n3\n2\n0\n...\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n9\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n203\n214\n166\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n10 rows × 785 columns\n\n\n\nWe can see that most of the data is 0 with few values here and there. That is because each row is a 28 * 28 pixel image flattened into an [784,] array. Each element con contain a value between 0 and 255. Furthermore, the target feature named as label is having values from 0 to 9. We will have to process this column a little bit, to make more sense in the context of fashion products that we need to predict. We will see how to do that in just a bit, but first, let us try to reshape the images into 28 * 28 and try to visualize them using the matplotlib subplots.\n\n# Visualize the Images in the Dataset\nlabels_list = list(fmnist_train_df['label'].unique())\nlabels_list.sort()\nfig, axs = plt.subplots(10, 10, figsize=(8, 8), layout='constrained')\nfor i in range(0, 10):\n    class_df = fmnist_train_df[fmnist_train_df['label'] == labels_list[i]]\n    for j in range(0, 10):\n        sample_image = class_df.iloc[j, 1:].values.reshape(28, 28)\n        axs[i][j].imshow(sample_image, cmap='Blues')\n        axs[i][j].grid(False)\n        if j == 0: axs[i][j].set_ylabel(f\"Class {labels_list[i]}\")\nplt.grid(False)\nplt.show()\n\n\n\n\nWe can see from the above plot different kinds of fashion products in the dataset. An obvious difficulty our multiclass classification algorith may face is the close resemblance of three different classes - class 0, class 2, and class 6. Fashion MNIST labels these classes as T-shirt/Top, Pullover, and Shirt respectively. To make target classes clear for us going forward, let us write a helper function that maps, the respective class number to its corresponding label.\n\n# Create more descriptive labels\ndef label_mapper(input):\n    match input:\n        case 0:\n            return 'T-shirt/top'\n        case 1:\n            return 'Trouser'\n        case 2:\n            return 'Pullover'\n        case 3:\n            return 'Dress'\n        case 4:\n            return 'Coat'\n        case 5:\n            return 'Sandal'\n        case 6:\n            return 'Shirt'\n        case 7:\n            return 'Sneaker'\n        case 8:\n            return 'Bag'\n        case 9:\n            return 'Ankle boot'\n\nfmnist_train_df['label'] = fmnist_train_df['label'].apply(lambda x:label_mapper(x))\nfmnist_test_df['label'] = fmnist_test_df['label'].apply(lambda x:label_mapper(x))\n\nLet us visualize if these changes took effect in the label column by printing a few values.\n\nfmnist_train_df['label'].head(10)\n\n0       Pullover\n1     Ankle boot\n2          Shirt\n3    T-shirt/top\n4          Dress\n5           Coat\n6           Coat\n7         Sandal\n8           Coat\n9            Bag\nName: label, dtype: object\n\n\n\n\n2. Prepare the data for classification\n\n# Split the data into predictors (features) and target (label) in train and test sets\nX_train = fmnist_train_df.iloc[:, 1:]\nX_test = fmnist_test_df.iloc[:, 1:]\ny_train = fmnist_train_df['label'].values\ny_test = fmnist_test_df['label'].values\n\nWe already have the training and test set split for us. So let us just separate the predictor and target variables into matrix X (actually pandas DataFrame) and vector y (a pandas series) from training and test set. We will beusing the X_train and y_train to fit a classification model. We will then be using y_test to make prediction on samples that are unseen by the trained multiclass classification model. Finally, we will use y_test to evaluate our model.\nBefore we get ahead to the model, we will need to do some preprocessing. Image classification is a computationally expensive task and training 60,000 images each of 28*28 pixels can take significant of computing resources and training time. Furthermore, we do not have to invest as much resources, because very often only a portion of the images are useful for training. Take for example, in the below image of a pullover, a RandomForestClassifier trained on training data of 1000 samples, showing the important pixels in the image.\n\nfrom sklearn.ensemble import RandomForestClassifier\nrnd_clf = RandomForestClassifier(n_estimators=100)\nrnd_clf.fit(X_train[:1000], y_train[:1000])\n\nheatmap_image = rnd_clf.feature_importances_.reshape(28, 28)\nplt.imshow(heatmap_image, cmap=\"hot\")\ncbar = plt.colorbar(ticks=[rnd_clf.feature_importances_.min(),\n                           rnd_clf.feature_importances_.max()])\ncbar.ax.set_yticklabels(['Not important', 'Very important'], fontsize=14)\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n2.1 Principal Component Analysis\nAn amazingly helpful strategy to reduce the number of features in the dataset is Principal Component Analysis (PCA). PCA is a dimensionality reduction technique, that transforms the original feature space into a reduced feature space of orthogonal components such that the transformed feature space retains as much variance as possible from the original space. This transformed coordinate system for the principal components. The first principal component can equivalently be defined as a direction that maximizes the variance of the projected data. The i-th principal component can be taken as a direction orthogonal to the first i-1 principal components that maximizes the variance of the projected data.\nIt can be shown that the principal components are eigenvectors of the data’s covariance matrix. Thus, the principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition (SVD) of the data matrix. In SVD, we will start from the original data matrix X which has samples in rows and measured variables in columns. The SVD decomposition breaks this matrix X into three matrices -\n\\[ X(n × p) = U_(n × p) * D_(p × p) * V^T_(p × p) \\]\nWhere, - X contains the original data - The columns of U are vectors giving the principal axes. These define the new coordinate system. - The scores can be obtained by XV; scores are the projections of the data on the principal axes. - D is a diagonal matrix, which means all non-diagonal elements are zero. The diagonal contains positive values sorted from largest to smallest. These are called the singular values. - The columns of V are the PCA loadings\nIn addition, U and V are semi-orthogonal matrices, which means that when pre-multiplied by their transpose one gets the identity matrix:\n\\[ U^T * U = I \\] \\[ V^T * V = I \\]\nTo reduce the number of features from 784, We will perform PCA on the fashion-mnist dataset. Before we do that we need to scale our data. PCA assumes that data is normally distributed and is sensitive to the variance of variables, so we scale the data to ensure all variables are on the same scale. This also improves the performance of the algorithm and its prediction accuracy.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\npca = PCA()\npca.fit(X_train)\ncumulative_variance = np.cumsum(pca.explained_variance_ratio_)\nncomponents_95_variance = np.argmax(cumulative_variance &gt;= 0.95) + 1\nprint(f\"Number of Components needed for 95% Variance - {ncomponents_95_variance}\")\n\nNumber of Components needed for 95% Variance - 256\n\n\nTo above step, creates an instance of StandardScaler and fits and transforms the trianing images. The test images are then scaled. The next part performs a full PCA analysis on the training set and calculates the cumulative explained variance - this is the sum of variance by individual principal components. As we can see, to retain 95% of the variance in the dataset, we need just 256 features. It is now computationally manageable for us to train the classifier using a subset of features. One drawback of PCA however is that, it does not tell us which features to retain and which to drop. That is why we will rely on a powerful classifier like RandomForestClassifier, which supports feature sampling.\nBefore that, let me show you a very useful plot that will allow you to visualize the number of components versus the explained variance. This plot allows you to see, how many components you need to retain certain amount of variance in the reduced feature space. For example, in the below plot you can see that, we need only 150 features to explain 91% of the variance in the data. We can build a pretty good model with that variance and strong classifier.\n\nplt.figure(figsize=(8, 6))\nplt.plot(np.arange(1, len(pca.explained_variance_ratio_)+1, 1), np.cumsum(pca.explained_variance_ratio_)*100)\nplt.plot([150, 150], [0, 91], \"k:\")\nplt.plot([0, 150], [91, 91], \"r:\")\nplt.plot(150, 91, \"ro\")\nplt.text(x=-20, y=91, s='91%', color='red', weight='bold', ha='center', va='center')\nplt.text(x=150, y=-3.5, s='150', color='blue', weight='bold', ha='center', va='center')\nplt.axis([0, 600, 0, 100])\nplt.title(\"Fashion MNIST PCA Analysis - Explained Variance vs # of Components\")\nplt.xlabel(\"Number of Components\", weight='bold')\nplt.ylabel(\"Explained Cumulative Variance (%)\", weight='bold')\nplt.xticks(weight='bold')\nplt.yticks(weight='bold')\nplt.grid(which='major', linestyle='-')\nplt.show()\n\n\n\n\n\n\n2.2 Hyperparameter Tuning with Randomized Search\nWe can also use hyperparameter tuning to search for best parameters. This is called Hyperparameter Optimization or Hyperparameter Tuning. Scikit learn’s GridSearchCV or RandomizedSearchCV can help with hyperparameter tuning. We can get the best values for number of components from principal component analysis and number of estimators from RandomForestClassifier using these methods. RandomizedSearchCV is preferred over GridSearchCV when there are many parameter values to try out. In contrast to GridSearchCV, RandomizedSearchCV tries not all parameter values but rather a fixed number of parameter settings sampled from the specified distributions making the search for best parameters quicker.\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestClassifier\n\npreprocess_pipeline = make_pipeline(PCA(random_state=5805), RandomForestClassifier(random_state=5805))\nparams_distribution = {\n    \"pca__n_components\": np.arange(50, 250), \n    \"randomforestclassifier__n_estimators\": np.arange(100, 400)\n}\nrandomized_search = RandomizedSearchCV(preprocess_pipeline, params_distribution, n_iter=10, cv=3, random_state=5805)\nrandomized_search.fit(X_train[:1000], y_train[:1000])\n\nRandomizedSearchCV(cv=3,\n                   estimator=Pipeline(steps=[('pca', PCA(random_state=5805)),\n                                             ('randomforestclassifier',\n                                              RandomForestClassifier(random_state=5805))]),\n                   param_distributions={'pca__n_components': array([ 50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,\n        63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,\n        76,  77,  78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,\n        89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99,...\n       308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320,\n       321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333,\n       334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346,\n       347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359,\n       360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372,\n       373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385,\n       386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398,\n       399])},\n                   random_state=5805)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=3,\n                   estimator=Pipeline(steps=[('pca', PCA(random_state=5805)),\n                                             ('randomforestclassifier',\n                                              RandomForestClassifier(random_state=5805))]),\n                   param_distributions={'pca__n_components': array([ 50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,\n        63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,\n        76,  77,  78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,\n        89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99,...\n       308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320,\n       321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333,\n       334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346,\n       347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359,\n       360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372,\n       373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385,\n       386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398,\n       399])},\n                   random_state=5805)estimator: PipelinePipeline(steps=[('pca', PCA(random_state=5805)),\n                ('randomforestclassifier',\n                 RandomForestClassifier(random_state=5805))])PCAPCA(random_state=5805)RandomForestClassifierRandomForestClassifier(random_state=5805)\n\n\nWe can then use the best_params_ attribute from the model to see the parameters that give best results. Note that above I have performed the search on 1000 training instances. Fell free to try and train on more instances. As you can see below, the number of estimators (desision tree classifiers) required by RandomForestClassifier are 197 and number of features are 234.\n\nrandomized_search.best_params_\n\n{'randomforestclassifier__n_estimators': 197, 'pca__n_components': 234}\n\n\n\n\n3. Train a RandomForestClassifier and visualize its performance metrics\nUsing this analysis and hyperparameters, let us now proceed to train the classifier. We will use the n_estimators in RandomForestClassifier to be 197 and fit method to train the classifier. We will use the predict method of the classifier to get the predictions the classifier will make on the dataset. Let us store them in y_pred which we can later use for evaluating the classifier.\n\n# Perform PCA Transform with above n_components\npca = PCA(n_components=234, random_state=5805)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\n\n\n# Train classifier\nrandom_forest_classifier = RandomForestClassifier(n_estimators=197, random_state=5805)\nrandom_forest_classifier.fit(X_train, y_train)\ny_pred = random_forest_classifier.predict(X_test)\n\nNow that we have trained our model and obtained the predictions from it for the test set. It is time to evaluate its performance. A very useful tool for evaluating the multiclass classification model is the classification_report from the sklearn metrics module. It gives us the precision, recall, and f1-score for each class. It also gives the overall f1-score of the classifier and a few other metrics we will discuss in just a second. We can extract the classification_report as follows -\n\nfrom sklearn.metrics import classification_report\n\nprint(\n    f\"Classification report for {random_forest_classifier}:\\n\"\n    f\"{classification_report(y_test, y_pred)}\\n\"\n)\n\nClassification report for RandomForestClassifier(n_estimators=197, random_state=5805):\n              precision    recall  f1-score   support\n\n  Ankle boot       0.89      0.94      0.92      1000\n         Bag       0.93      0.97      0.95      1000\n        Coat       0.79      0.87      0.83      1000\n       Dress       0.88      0.92      0.90      1000\n    Pullover       0.80      0.79      0.80      1000\n      Sandal       0.93      0.90      0.92      1000\n       Shirt       0.74      0.57      0.65      1000\n     Sneaker       0.90      0.88      0.89      1000\n T-shirt/top       0.79      0.84      0.82      1000\n     Trouser       0.99      0.96      0.98      1000\n\n    accuracy                           0.87     10000\n   macro avg       0.86      0.87      0.86     10000\nweighted avg       0.86      0.87      0.86     10000\n\n\n\n\nSo how do we read this classification report? Let us first define each of the metrics.\n\nPrecision - In a multi-class classification task, the precision for a class is the number of true positives (i.e. the number of images correctly labelled as belonging to the positive class) divided by the total number of elements labelled as belonging to the positive class (i.e. the sum of true positives and false positives, which are images incorrectly labelled as belonging to the class).\n\n\\[ {\\displaystyle \\mathrm {PPV} ={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FP} }}=1-\\mathrm {FDR} } \\]\n\nRecall - Recall in this context is defined as the number of true positives divided by the total number of images that actually belong to the positive class (i.e. the sum of true positives and false negatives, which are images which were not labelled as belonging to the positive class but should have been).\n\n\\[ {\\displaystyle \\mathrm {TPR} ={\\frac {\\mathrm {TP} }{\\mathrm {P} }}={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FN} }}=1-\\mathrm {FNR} } \\]\n\nPrecision and recall are not particularly useful metrics when used in isolation. The need to be used in conjunction with other metrics to obtain a comprehensive evaluation of the model’s performance. For instance, A trivial way to have perfect precision is to create a classifier that always makes negative predictions, except for one single positive prediction on the instance it’s most confident about. If this one prediction is correct, then the classifier has 100% precision. Such a classifer would not be any helpful. Likewise, it is possible to have perfect recall by simply retrieving every single item. - F1-score and Accuracy - The F1-score is a metric that combines precision and recall to obtain a balanced classification model. The F1 score favors classifiers that have similar precision and recall. It is calculated as the harmonic mean of precision and recall. Accuracy is another metric that measures the proportion of correct predictions made by the model.\n\nF1 Score \\[ {\\displaystyle \\mathrm {F} _{1}=2\\times {\\frac {\\mathrm {PPV} \\times \\mathrm {TPR} }{\\mathrm {PPV} +\\mathrm {TPR} }}={\\frac {2\\mathrm {TP} }{2\\mathrm {TP} +\\mathrm {FP} +\\mathrm {FN} }}} \\]\nAccuracy (ACC) \\[ {\\displaystyle \\mathrm {ACC} ={\\frac {\\mathrm {TP} +\\mathrm {TN} }{\\mathrm {P} +\\mathrm {N} }}={\\frac {\\mathrm {TP} +\\mathrm {TN} }{\\mathrm {TP} +\\mathrm {TN} +\\mathrm {FP} +\\mathrm {FN} }}} \\]\nAnathor important tool to evaluate the classifier is the Confusion Matrix, in the multi-class classification context, it gives the actual predictions in horizontal rows and predicted classes in vertical columns. We can use the ConfusionMatrixDisplay from scikit learn’s metrics module to plot the confusion matrix for the classifier.\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\ndisp = ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\ndisp.figure_.suptitle(\"Confusion Matrix\")\nprint(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\nplt.grid(False)\nplt.xticks(rotation=60)\nplt.show()\n\nConfusion matrix:\n[[944   0   0   0   0  20   0  36   0   0]\n [  0 975   4   4   4   2   7   2   2   0]\n [  0   8 868  24  51   0  49   0   0   0]\n [  0   3  24 922   8   0  14   0  24   5]\n [  0  13 117  10 788   1  57   0  14   0]\n [ 36   6   0   0   0 902   0  56   0   0]\n [  0  25  82  23 113   0 575   0 182   0]\n [ 76   1   0   0   0  41   0 882   0   0]\n [  1  20   4  40  13   5  71   1 844   1]\n [  1   1   3  20   4   0   4   0   3 964]]\n\n\n\n\n\nWe can normalize the confusion matrix by dividing each value by the total number of images in the corresponding (true) class (i.e., divide by the row’s sum). This helps us to see where the model has misclassfied the images of a given class as belonging to different class.\n\n# Plot the normalized confused matrix.\nConfusionMatrixDisplay.from_predictions(y_test, y_pred, normalize='true', values_format=\".0%\")\nplt.grid(False)\nplt.title(\"Normalized Confusion Matrix\")\nplt.xticks(rotation=60)\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n4. Observations and Conclusion\nLooking at the classification report, ROC Curve, and confusion matrix, we can make following observations -\nThe precision for “coat”, “shirt”, “pullover”, and “T-shirt/top” is very low. This tells us that the model is predicting many False Positives for these classes. This is because these four classes are closely resembling one another. The classifier is unable to tell apart confidently a “coat” from a “shirt” and is resulting in many false positives and low precision. One way to improve the model precision for these labels is to increase the number of samples so that model can learn the subtle differences to tell apart each of these classes.\nThe recall for shirt is very low 57%. This tells that many images of shirt have been misclassified as belonging to a different class. Looking at the normalized confusin matrix, we can see that 18% of shirts have been misclassified as T-Shirt/Top, 11% of the shirts have been misclassified as pullover and 8% as Coats. We can see the same issue with images of pullover and coat, they have been misclassified as these other similar looking products. Other ways to improve the performance of our classification model is by doing searching for a better set of hyperparameters and re-training the model to improve its performance across all class labels.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html",
    "href": "posts/Probability Theory and Random Variables/index.html",
    "title": "Probability Theory - Gaussian Distribution, Normality Tests, and Z-Scores",
    "section": "",
    "text": "In this blogpost, we will be discussing an important topic in statistics and machine learning - the Gaussian Distribution, also called the Normal Distribution. It is used to model many natural phenomena in the world, from people’s heights to size of snowflakes, errors in measurements, and other financial and forecasting data. We will talk about why gaussian distribution is important in machine learning and how many ML algorithms assume that underlying data is normally distributed. We will explore some graphical and numerical methods to perform normality tests on data. Finally, we will try to transform the data to standard normal distribution by calculating z-scores. At a high level, we will explore -\nWe will be working with the iris dataset throughout this blogpost."
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html#normal-distribution-and-machine-learning",
    "href": "posts/Probability Theory and Random Variables/index.html#normal-distribution-and-machine-learning",
    "title": "Probability Theory - Gaussian Distribution, Normality Tests, and Z-Scores",
    "section": "1. Normal Distribution and Machine Learning",
    "text": "1. Normal Distribution and Machine Learning\nA normal distribution is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. It is also known as the Gaussian distribution. The standard normal distribution is a normal distribution with zero mean and unit variance. The normal distribution has several key features and properties that define it. First, its mean (average), median (midpoint), and mode (most frequent observation) are all equal to one another. Moreover, these values all represent the peak, or highest point, of the distribution. The distribution then falls symmetrically around the mean, the width of which is defined by the standard deviation.\nAll normal distributions can be described by just two parameters: the mean and the standard deviation. The Empirical Rule states that for all normal distributions, 68.2% of the observations will appear within plus or minus one standard deviation of the mean; 95.4% of the observations will fall within +/- two standard deviations; and 99.7% within +/- three standard deviations. This fact is sometimes referred to as the “empirical rule,” a heuristic that describes where most of the data in a normal distribution will appear. This means that data falling outside of three standard deviations (“3-sigma”) would signify rare occurrences\n\n\n\nStandard Normal Distribution\n\n\nMachine learning models can be classified into two categories: parametric and non-parametric methods.\nParametric methods are those that require the specification of some parameters before they can be used to make predictions. These models make assumptions about the distribution of the data, and the parameters are estimated from the training data. They are generally simpler and faster to train and do not require huge amounts of data contrary to non-parametric methods. Non-parametric models do not make any assumptions about the distribution of the data and instead rely on the data itself to determine the model structure, they may be slower to train and require more data to achieve good performance.\nIt is usually good practice to first try parametric models like Linear Regression, Logistic Regression, Linear Discriminant Analysis (LDA), and Gaussian Naive Bias before exploring other non-parametric methods and advanced models. Furthermore, converting the data into a normal distribution allows for fair comparisons of features with different distributions and scales and improve the accuracy of predictions.\nLet us start importing some packages and exploring the diamonds dataset and its features.\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy\nimport warnings\n\nwarnings.simplefilter('ignore', UserWarning)\niris = sns.load_dataset('iris')\niris.head(10)\n# iris.columns #'sepal_length', 'sepal_width', 'petal_length', 'petal_width','species'\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n6\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n7\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n8\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n9\n4.9\n3.1\n1.5\n0.1\nsetosa\n\n\n\n\n\n\n\n\nplt.figure()\nsns.histplot(iris, x='sepal_length', kde=True)\nplt.title(\"Distribution of \\\"sepal_length\\\" Feature\")\nplt.show()\n\n\n\n\nA variable that is normally distributed has a histogram (or “density function”) that is bell-shaped, with only one peak, and is symmetric around the mean. The sepal_length feature does seem to resember the bell-shaped curve and can be normally distributed, lets us explore the other numeric features as well.\n\ndef iris_histogram_plotter(df):\n    features_list = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n    counter = 0\n    fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(8, 8))\n    for i in range(2):\n        for j in range(2):\n            sns.histplot(df, x=features_list[counter], kde=True, ax=axs[i][j])\n            axs[i][j].set_title(f\"Distribution of \\\"{features_list[counter]}\\\" Feature\")\n            counter += 1\n    plt.tight_layout()\n    plt.show()\n\niris_histogram_plotter(df=iris)\n\n\n\n\nWe can see that amongst the four sepal_length and sepal_width features seem normally distributed. However, we will need to use more rigid tests to assess the normality of these features."
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html#normality-tests",
    "href": "posts/Probability Theory and Random Variables/index.html#normality-tests",
    "title": "Probability Theory - Gaussian Distribution, Normality Tests, and Z-Scores",
    "section": "2. Normality Tests",
    "text": "2. Normality Tests\nA normality test is used to determine whether sample data has been drawn from a normally distributed population (within some tolerance). We can use graphical/visual methods or normality tests to assess normality of the data.\n\n2.1 Graphical Methods\nAlthough somewhat unreliable and does not always guarantee that the distribution is normal, visual inspection is a very helpful step in assessing normality. The frequency distributions (histogram) we plotted earlier are one of the visual techniques. Other methods like stem-and-leaf plot, boxplot, P-P (probability-probability) plot, and Q-Q plot (quantile-quantile plot) are used for checking normality visually. We already explored histogram plot, we will plot Q-Q plot to test normality of numerical features in iris dataset. If the data is normally distributed for a feature, the points will fall on the 45-degree reference line.\n\nfrom statsmodels.graphics.gofplots import qqplot\n\nfeatures_list = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\nfig = plt.figure(figsize=(8, 8))\nax1 = fig.add_subplot(221)\nqqplot(iris['sepal_length'], line='s', ax=ax1)\nax1.set_title(\"QQ-plot - Normality Test for \\\"sepal_length\\\" feature\")\nax1.grid()\n\nax2 = fig.add_subplot(222)\nqqplot(iris['petal_length'], line='s', ax=ax2)\nax2.set_title(\"QQ-plot - Normality Test for \\\"petal_length\\\" feature\")\nax2.grid()\n\nax1 = fig.add_subplot(223)\nqqplot(iris['sepal_width'], line='s', ax=ax1)\nax1.set_title(\"QQ-plot - Normality Test for \\\"sepal_width\\\" feature\")\nax1.grid()\n\nax2 = fig.add_subplot(224)\nqqplot(iris['petal_width'], line='s', ax=ax2)\nax2.set_title(\"QQ-plot - Normality Test for \\\"petal_width\\\" feature\")\nax2.grid()\n\nplt.show()\n\n\n\n\nOnce again, we can see sepal length and width closely follow the standardized line while the petal length and petal width do not fit properly against the reference normal distribution.\n\n\n2.2 Statistical Normality Tests\nThe Statistical Normality Tests are supplementary to the graphical assessment of normality. Read here for a complete list of normality tests. The main tests for the assessment of normality are -\nKolmogorov-Smirnov (K-S) test or KS test is a nonparametric test of equality of a continuous one-dimensional probability distributions. It can be used to compare a sample with a reference probability distribution (like the samples drawn from normal distribution) or to compare two samples. The Kolmogorov–Smirnov statistic quantifies a distance between the empirical distribution function of the sample and the cumulative distribution function of the reference normal distribution. The empirical distribution function \\(F_n\\) for n independent and identically distributed ordered observations \\(X_i\\) is defined as -\n\\[ {\\displaystyle F_{n}(x)={\\frac {{\\text{number of (elements in the sample}}\\leq x)}{n}}={\\frac {1}{n}}\\sum _{i=1}^{n}1_{(-\\infty ,x]}(X_{i}),} \\]\nA limitation of the K-S test is its high sensitivity to extreme values.\nShapiro-Wilk test is based on the correlation between the data and the corresponding normal scores and provides better power than the K-S test. The test works by ordering and standardizing the samples \\(\\mu=0\\) and \\(\\sigma=1\\). The test statistic is given by the formula -\n\\[ {\\displaystyle W={\\left(\\sum _{i=1}^{n}a_{i}x_{(i)}\\right)^{2} \\over \\sum _{i=1}^{n}(x_{i}-{\\overline {x}})^{2}},} \\]\nOne disadvantage is the Shapiro–Wilk test is known not to work well in samples with many identical values.\nD’Agostino’s K-squared test is a statistical test that measures how well a given dataset fits a normal distribution. The test is based on the sample skewness and kurtosis, which are measures of the asymmetry and peakedness of the data, respectively. The test statistic is calculated as the sum of the squares of the standardized deviations of the sample skewness and kurtosis from their expected values under the assumption of normality. The test statistic is then compared to a chi-squared distribution with two degrees of freedom, and the p-value is calculated. If the p-value is less than the significance level, then the null hypothesis that the data is normally distributed is rejected. Otherwise, the null hypothesis is not rejected.\nD’Agostino’s K-squared test is generally preferred over Shapiro-Wilk when the sample size is large.\nLet us import these methods from scipy.stats module, to test the normality of our iris dataset features.\n\nfrom scipy.stats import kstest, shapiro, normaltest\n\nnp.random.seed(5805)\ndef calc_ks_test_statistics(sample):\n    sample_mean = np.mean(sample)\n    sample_sd = np.std(sample)\n    reference_normal_dist = np.random.normal(sample_mean, sample_sd, len(sample))\n    statistic, p_value = kstest(sample, reference_normal_dist)\n    return statistic, p_value\n\ndef calc_shapiro_test_statistics(sample):\n        statistic, p_value = shapiro(sample)\n        return statistic, p_value\n\ndef calc_dk2_test_statistics(sample):\n        statistic, p_value = normaltest(sample)\n        return statistic, p_value\n\n\ndef normality_test(df, feature_list, tolerance):\n    for feature in feature_list:\n        print('=' * 100)\n        ks_statistic, ks_pvalue = calc_ks_test_statistics(df[feature])\n        print(f\"Kolmogorov–Smirnov test: The test statistic and p-value of {feature} are {ks_statistic:.3f} and {ks_pvalue:.3f}\")\n        if ks_pvalue &lt; tolerance:\n            print(f\"According to KS-Test the feature {feature} is NOT NORMALLY DISTRIBUTED.\")\n        else:\n            print(f\"According to KS-Test the feature {feature} is NORMALLY DISTRIBUTED!\")\n        print('=' * 100)\n        print()\n\n        print('=' * 100)\n        shapiro_statistic, shapiro_pvalue = calc_shapiro_test_statistics(df[feature])\n        print(f\"Shapiro-Wilk Test: The test statistic and p-value of {feature} are {shapiro_statistic:.3f} and {shapiro_pvalue:.3f}\")\n        if shapiro_pvalue &lt; tolerance:\n            print(f\"According to Shapiro-Wilk the feature {feature} is NOT NORMALLY DISTRIBUTED.\")\n        else:\n            print(f\"According to Shapiro-Wilk the feature {feature} is NORMALLY DISTRIBUTED!\")\n        print('=' * 100)\n        print()\n\n\n        print('=' * 100)\n        dk2_statistic, dk2_pvalue = calc_dk2_test_statistics(df[feature])\n        print(f\"D’Agostino-Pearson Test: The test statistic and p-value of {feature} are {dk2_statistic:.3f} and {dk2_pvalue:.3f}\")\n        if dk2_pvalue &lt; tolerance:\n            print(f\"According to D’Agostino-Pearson test the feature {feature} is NOT NORMALLY DISTRIBUTED.\")\n        else:\n            print(f\"According to D’Agostino-Pearson test the feature {feature} is NORMALLY DISTRIBUTED!\")\n        print('=' * 100)\n        print()\n\nfeatures_list = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\nnormality_test(df=iris, feature_list=features_list, tolerance=0.05)\n\n====================================================================================================\nKolmogorov–Smirnov test: The test statistic and p-value of sepal_length are 0.080 and 0.725\nAccording to KS-Test the feature sepal_length is NORMALLY DISTRIBUTED!\n====================================================================================================\n\n====================================================================================================\nShapiro-Wilk Test: The test statistic and p-value of sepal_length are 0.976 and 0.010\nAccording to Shapiro-Wilk the feature sepal_length is NOT NORMALLY DISTRIBUTED.\n====================================================================================================\n\n====================================================================================================\nD’Agostino-Pearson Test: The test statistic and p-value of sepal_length are 5.736 and 0.057\nAccording to D’Agostino-Pearson test the feature sepal_length is NORMALLY DISTRIBUTED!\n====================================================================================================\n\n====================================================================================================\nKolmogorov–Smirnov test: The test statistic and p-value of sepal_width are 0.173 and 0.022\nAccording to KS-Test the feature sepal_width is NOT NORMALLY DISTRIBUTED.\n====================================================================================================\n\n====================================================================================================\nShapiro-Wilk Test: The test statistic and p-value of sepal_width are 0.985 and 0.101\nAccording to Shapiro-Wilk the feature sepal_width is NORMALLY DISTRIBUTED!\n====================================================================================================\n\n====================================================================================================\nD’Agostino-Pearson Test: The test statistic and p-value of sepal_width are 3.124 and 0.210\nAccording to D’Agostino-Pearson test the feature sepal_width is NORMALLY DISTRIBUTED!\n====================================================================================================\n\n====================================================================================================\nKolmogorov–Smirnov test: The test statistic and p-value of petal_length are 0.213 and 0.002\nAccording to KS-Test the feature petal_length is NOT NORMALLY DISTRIBUTED.\n====================================================================================================\n\n====================================================================================================\nShapiro-Wilk Test: The test statistic and p-value of petal_length are 0.876 and 0.000\nAccording to Shapiro-Wilk the feature petal_length is NOT NORMALLY DISTRIBUTED.\n====================================================================================================\n\n====================================================================================================\nD’Agostino-Pearson Test: The test statistic and p-value of petal_length are 221.687 and 0.000\nAccording to D’Agostino-Pearson test the feature petal_length is NOT NORMALLY DISTRIBUTED.\n====================================================================================================\n\n====================================================================================================\nKolmogorov–Smirnov test: The test statistic and p-value of petal_width are 0.180 and 0.015\nAccording to KS-Test the feature petal_width is NOT NORMALLY DISTRIBUTED.\n====================================================================================================\n\n====================================================================================================\nShapiro-Wilk Test: The test statistic and p-value of petal_width are 0.902 and 0.000\nAccording to Shapiro-Wilk the feature petal_width is NOT NORMALLY DISTRIBUTED.\n====================================================================================================\n\n====================================================================================================\nD’Agostino-Pearson Test: The test statistic and p-value of petal_width are 137.556 and 0.000\nAccording to D’Agostino-Pearson test the feature petal_width is NOT NORMALLY DISTRIBUTED.\n====================================================================================================\n\n\n\nWe notice that all three tests are in perfect agreement on petal length and petal width. These two features are definitely not normally distributed. However, the tests vary in their agreements on whether sepal length and width is normal or not. Both KS-Test and D’Agostino-Pearson say that sepal_length is normally distributed at 95% confidence interval. Likewise, Shapiro-Wilk and D’Agostino-Pearson say that sepal width is normally distributed. These disagreements are typically because of the size of the dataset and also presence of many identical values in the response variable we are working with."
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html#transforming-data-to-standard-normal-distribution-z-scores",
    "href": "posts/Probability Theory and Random Variables/index.html#transforming-data-to-standard-normal-distribution-z-scores",
    "title": "Probability Theory - Gaussian Distribution, Normality Tests, and Z-Scores",
    "section": "3. Transforming Data to Standard Normal Distribution: Z-Scores",
    "text": "3. Transforming Data to Standard Normal Distribution: Z-Scores\nA z-score, or standard score, is used for standardizing scores on the same scale by dividing a score’s deviation by the standard deviation in a data set. The result is a standard score. It measures the number of standard deviations that a given data point is from the mean. You would use z-score to ensure your feature distributions have mean = 0 and std = 1.\n\ndef transform_iris_normal():\n    iris_scaled = iris.copy()\n    for feature in features_list:\n        feature_mean = np.mean(iris_scaled[feature])\n        feature_std = np.std(iris_scaled[feature])\n        iris_scaled[feature] = iris_scaled[feature].apply(lambda f: (f - feature_mean) / feature_std)\n    return iris_scaled\n\niris_scaled_df = transform_iris_normal()\niris_histogram_plotter(df=iris_scaled_df)\n\n\n\n\nThe features are now transformed and each value indicate the number of standard deviations that a given observation is above or below the mean.\n\nmean = np.mean(iris_scaled_df['sepal_length'].round(2))\nsd = np.std(iris_scaled_df['sepal_length'].round(2))\nprint(f\"Sepal Length Mean after Z-Score Transformation : {mean:.2f} \")\nprint(f\"Sepal Length Standard Deviation after Z-Score Transformation : {sd:.2f}\")\n\nSepal Length Mean after Z-Score Transformation : 0.00 \nSepal Length Standard Deviation after Z-Score Transformation : 1.00"
  }
]