{
  "hash": "a2088de3ebf8d91cc3238d5406f5b360",
  "result": {
    "markdown": "---\ntitle: Predicting Continuous Variables using Regression Analysis - Linear and Non-Linear Regression\nauthor: Sai Sundeep Rayidi\ndate: '2023-12-05'\ncategories:\n  - Linear Regression\n  - Non-Linear Regression\nimage: regression.png\nformat:\n  html:\n    code-fold: false\n    code-summary: Show the code\n---\n\nIn this blogpost we will be getting to know some important and widely utilized machine learning models for estimating continuous variables - Regression analysis. Regression analysis can be used to estimate the value of one variable using the known values of other variables and predict results and shifts in a variable based on its relationship with other variables. For instance, regression analysis can be used to predict the sales of a product based on its price, advertising, and other factors. We will be learning the following concepts in this blog -\n\n1. Analyzing the **Diamonds** Dataset\n2. Linear Regression and Normal Equation\n3. Modeling non-linear relationships using Random Forests \n\n\n## 1. Analyzing the Diamonds Dataset\n\nLet us start by importing some packages and loading the Diamonds dataset available in the Seaborn visualization package. We can get the high level overview of the dataset by exploring it using the   shape argument and describe method. We can see the first few records calling the head method on the diamonds dataframe. \n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-12-07T21:22:43.390734300Z\",\"start_time\":\"2023-12-07T21:22:43.333273800Z\"}' execution_count=1}\n``` {.python .cell-code}\n#Import Packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nsns.set_theme(style=\"darkgrid\")\ndiamonds = sns.load_dataset('diamonds')\nprint(f\"Number of Observations: {diamonds.shape[0]}\")\nprint(f\"Number of Features: {diamonds.shape[1]}\")\nprint(f\"\\nSummary Statistics of numerical features:\\n {diamonds.describe()}\")\nprint(f\"\\nFew Sample Records: \\n{diamonds.head(10)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of Observations: 53940\nNumber of Features: 10\n\nSummary Statistics of numerical features:\n               carat         depth         table         price             x  \\\ncount  53940.000000  53940.000000  53940.000000  53940.000000  53940.000000   \nmean       0.797940     61.749405     57.457184   3932.799722      5.731157   \nstd        0.474011      1.432621      2.234491   3989.439738      1.121761   \nmin        0.200000     43.000000     43.000000    326.000000      0.000000   \n25%        0.400000     61.000000     56.000000    950.000000      4.710000   \n50%        0.700000     61.800000     57.000000   2401.000000      5.700000   \n75%        1.040000     62.500000     59.000000   5324.250000      6.540000   \nmax        5.010000     79.000000     95.000000  18823.000000     10.740000   \n\n                  y             z  \ncount  53940.000000  53940.000000  \nmean       5.734526      3.538734  \nstd        1.142135      0.705699  \nmin        0.000000      0.000000  \n25%        4.720000      2.910000  \n50%        5.710000      3.530000  \n75%        6.540000      4.040000  \nmax       58.900000     31.800000  \n\nFew Sample Records: \n   carat        cut color clarity  depth  table  price     x     y     z\n0   0.23      Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n1   0.21    Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n2   0.23       Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n3   0.29    Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n4   0.31       Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75\n5   0.24  Very Good     J    VVS2   62.8   57.0    336  3.94  3.96  2.48\n6   0.24  Very Good     I    VVS1   62.3   57.0    336  3.95  3.98  2.47\n7   0.26  Very Good     H     SI1   61.9   55.0    337  4.07  4.11  2.53\n8   0.22       Fair     E     VS2   65.1   61.0    337  3.87  3.78  2.49\n9   0.23  Very Good     H     VS1   59.4   61.0    338  4.00  4.05  2.39\n```\n:::\n:::\n\n\nLet us now explore the unique values in the 'cut', 'clarity', and 'color' features of the diamonds dataset, along with the unique number of observations in each class.  \n\nprint(\"\\nUnique clarity types and count of diamonds of each clarity type:\\n\")\nprint(diamonds['clarity'].value_counts(ascending=False))\nprint(\"\\nUnique colors and count of diamonds of each color:\\n\")\nprint(diamonds['color'].value_counts(ascending=False))\nprint(\"\\nUnique cut types and count of diamonds in each cut type:\\n\")\nprint(diamonds['cut'].value_counts(ascending=False))\n\nThere are eight different clarity types, seven different unique colors and five different cut types. It appears the sale of Ideal cut type is highest, with 21,551 diamonds sold. The highest sold clarity diamond is of type 13065, and color is 11,292.\n\nWe will try to predict the price of the diamonds given the other independent variables like its weight (carat), color, cut, clarity, and other dimensions. So, let us see what is the correlation between the response variable price and other predictor variables.  \n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-12-07T22:00:21.785496200Z\",\"start_time\":\"2023-12-07T22:00:21.762108900Z\"}' execution_count=2}\n``` {.python .cell-code}\ncorr_matrix = diamonds.select_dtypes(np.number).corr()\nprint(\"Correlation between price and other features: \\n\")\nprint(corr_matrix['price'].sort_values(ascending=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorrelation between price and other features: \n\nprice    1.000000\ncarat    0.921591\nx        0.884435\ny        0.865421\nz        0.861249\ntable    0.127134\ndepth   -0.010647\nName: price, dtype: float64\n```\n:::\n:::\n\n\nIt appears that price is most correlated to carat variable, followed by other dimensions x, y, and z. Let us visualize the scatter matrix of the features in our dataset as well as a scatter plot between carat and price to see how the relationship between the features looks graphically. \n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-12-07T22:07:47.744495300Z\",\"start_time\":\"2023-12-07T22:07:41.772531200Z\"}' execution_count=3}\n``` {.python .cell-code}\nfrom pandas.plotting import scatter_matrix\nscatter_matrix(diamonds[['price', 'carat', 'x', 'y', 'z', 'table', 'depth']], figsize=(10, 10))\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=825 height=821}\n:::\n:::\n\n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-12-07T22:07:47.918302800Z\",\"start_time\":\"2023-12-07T22:07:47.744495300Z\"}' execution_count=4}\n``` {.python .cell-code}\nplt.figure(figsize=(8, 8))\ndiamonds.plot(kind=\"scatter\", x=\"price\", y=\"carat\", grid=True)\nplt.title(\"Scatter plot between price and carat\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n```\n<Figure size 768x768 with 0 Axes>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-2.png){width=581 height=455}\n:::\n:::\n\n\n## 2. Linear Regression and the Normal Equation\n\nA linear regression model makes a prediction by calculating the weighted sum of the input features and a constant term called bias term. To put it formally, give a dataset of n observations denoted by ${\\{y_{i},\\,x_{i1},\\ldots ,x_{ip}\\}_{i=1}^{n}$ the linear regression model assumes that the relationship between the dependent variable y (also called a regressand or response variable) and the independent variables (also called regressors or predictor variables) is linear. Thus the simple linear regression model takes the form -\n\n$$\\large {\\displaystyle y_{i}=\\beta _{0}+\\beta _{1}x_{i1}+\\cdots +\\beta _{p}x_{ip}+\\varepsilon _{i}=\\mathbf {x} _{i}^{\\mathsf {T}}{\\boldsymbol {\\beta }}+\\varepsilon _{i},\\qquad i=1,\\ldots ,n,} $$\n\nWhere, \n$\\epsilon$ is the error term or noise term which is there to explain the influence of all other factors other than regressors $x$.\n$\\beta_{j}$ is the jth model parameter, also called the feature weights\n\nTo train a regression model that can predict y values given x. We will need to find the values of $\\beta$ that minimize the root mean squared error (MSE).    \n\n$$ {\\displaystyle {\\vec {\\hat {\\beta }}}={\\underset {\\vec {\\beta }}{\\mbox{arg min}}}\\,L\\left(D,{\\vec {\\beta }}\\right)={\\underset {\\vec {\\beta }}{\\mbox{arg min}}}\\sum _{i=1}^{n}\\left({\\vec {\\beta }}\\cdot {\\vec {x_{i}}}-y_{i}\\right)^{2}} $$   \n\nBy putting in the dependent and independent variables and finding the gradient of this function, one can arrive at the equation of best parameters, by setting the gradient to zero -\n\n$$\\large {\\vec {\\hat {\\beta }}} = \\left(X^{\\textsf {T}}X\\right)^{-1}X^{\\textsf {T}}Y$$\n\nThis is called the *Normal Equation*.\n\nUsing Scikit Learn we can perform all these steps simply by calling the LinearRegression class with the predictor and response variables. However, we first need to encode our various categorical and numerical features. Lest us first build a pre-processing pipeline to do the same.  \n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-12-07T23:52:01.026141500Z\",\"start_time\":\"2023-12-07T23:52:00.798902Z\"}' execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_features = ['cut', 'color', 'clarity']\nnumeric_features = ['x', 'y', 'z', 'carat', 'depth', 'table']\n\nnumeric_transformer = make_pipeline(SimpleImputer(strategy='median'), StandardScaler())\n\npreprocessor = ColumnTransformer(\n    [\n        ('num', numeric_transformer, numeric_features),\n        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),\n    ],\n    verbose_feature_names_out=False\n)\n```\n:::\n\n\nWe have built our pre-processing pipeline that will take care of both numerical and categorical features in our dataset. It will impute any missing values in the numerical columns and scale the features using StandardScaler(). In the case of categorical features, it will encode each unique value in carat, color, and cut to a numerical value. Let us now split the dataset into a train and test set for training and validation steps and train a linear regression model \n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-12-07T23:58:06.573538800Z\",\"start_time\":\"2023-12-07T23:58:06.417763800Z\"}' execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\ndiamonds_features, diamonds_price = diamonds.loc[:, diamonds.columns != 'price'], diamonds['price']\n\ndiamonds_features_train, diamonds_features_test, diamonds_price_train, diamonds_price_test = train_test_split(diamonds_features, diamonds_price, test_size=0.25)\n\nlin_reg = make_pipeline(preprocessor, LinearRegression())\nlin_reg.fit(diamonds_features_train, diamonds_price_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n                 ColumnTransformer(transformers=[(&#x27;num&#x27;,\n                                                  Pipeline(steps=[(&#x27;simpleimputer&#x27;,\n                                                                   SimpleImputer(strategy=&#x27;median&#x27;)),\n                                                                  (&#x27;standardscaler&#x27;,\n                                                                   StandardScaler())]),\n                                                  [&#x27;x&#x27;, &#x27;y&#x27;, &#x27;z&#x27;, &#x27;carat&#x27;,\n                                                   &#x27;depth&#x27;, &#x27;table&#x27;]),\n                                                 (&#x27;cat&#x27;,\n                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n                                                                sparse_output=False),\n                                                  [&#x27;cut&#x27;, &#x27;color&#x27;, &#x27;clarity&#x27;])],\n                                   verbose_feature_names_out=False)),\n                (&#x27;linearregression&#x27;, LinearRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n                 ColumnTransformer(transformers=[(&#x27;num&#x27;,\n                                                  Pipeline(steps=[(&#x27;simpleimputer&#x27;,\n                                                                   SimpleImputer(strategy=&#x27;median&#x27;)),\n                                                                  (&#x27;standardscaler&#x27;,\n                                                                   StandardScaler())]),\n                                                  [&#x27;x&#x27;, &#x27;y&#x27;, &#x27;z&#x27;, &#x27;carat&#x27;,\n                                                   &#x27;depth&#x27;, &#x27;table&#x27;]),\n                                                 (&#x27;cat&#x27;,\n                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n                                                                sparse_output=False),\n                                                  [&#x27;cut&#x27;, &#x27;color&#x27;, &#x27;clarity&#x27;])],\n                                   verbose_feature_names_out=False)),\n                (&#x27;linearregression&#x27;, LinearRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">columntransformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;,\n                                 Pipeline(steps=[(&#x27;simpleimputer&#x27;,\n                                                  SimpleImputer(strategy=&#x27;median&#x27;)),\n                                                 (&#x27;standardscaler&#x27;,\n                                                  StandardScaler())]),\n                                 [&#x27;x&#x27;, &#x27;y&#x27;, &#x27;z&#x27;, &#x27;carat&#x27;, &#x27;depth&#x27;, &#x27;table&#x27;]),\n                                (&#x27;cat&#x27;,\n                                 OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n                                               sparse_output=False),\n                                 [&#x27;cut&#x27;, &#x27;color&#x27;, &#x27;clarity&#x27;])],\n                  verbose_feature_names_out=False)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>[&#x27;x&#x27;, &#x27;y&#x27;, &#x27;z&#x27;, &#x27;carat&#x27;, &#x27;depth&#x27;, &#x27;table&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat</label><div class=\"sk-toggleable__content\"><pre>[&#x27;cut&#x27;, &#x27;color&#x27;, &#x27;clarity&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse_output=False)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div></div></div>\n```\n:::\n:::\n\n\nWe can make now make predictions using this trained model on our test samples. We can also visualize the coefficients (weights/parmaeters) of the features using the coef_ argument of the model. \n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-12-08T00:18:22.083901400Z\",\"start_time\":\"2023-12-08T00:18:22.068192100Z\"}' execution_count=7}\n``` {.python .cell-code}\ndiamonds_price_predictions = lin_reg.predict(diamonds_features_test)\nprint(f\"Weights/Coefficients of predictors:\\n {lin_reg[1].coef_}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWeights/Coefficients of predictors:\n [-1.56231024e+03  5.27637997e+02 -5.37870482e+01  5.26055155e+03\n -8.86441962e+01 -6.35762425e+01 -8.69307213e+14 -8.69307213e+14\n -8.69307213e+14 -8.69307213e+14 -8.69307213e+14  1.09282256e+15\n  1.09282256e+15  1.09282256e+15  1.09282256e+15  1.09282256e+15\n  1.09282256e+15  1.09282256e+15 -3.01470391e+15 -3.01470391e+15\n -3.01470391e+15 -3.01470391e+15 -3.01470391e+15 -3.01470391e+15\n -3.01470391e+15 -3.01470391e+15]\n```\n:::\n:::\n\n\nWe can see the performance of our model using these predictions and actual response variable values. The ***mean_squared_error*** and ***r2_score*** (also called coefficient of determination) tells us how well the model has performed on the test set. The R2 score provides the information about the goodness of fit of a model, that is, how well a regression line approximates the actual data. \n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-12-08T00:36:39.611417Z\",\"start_time\":\"2023-12-08T00:36:39.595820100Z\"}' execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nprint(\"Mean Squared Error: %.2f\" % mean_squared_error(diamonds_price_test, diamonds_price_predictions))\nprint(\"Coefficient of Determination: %.2f\" % r2_score(diamonds_price_test, diamonds_price_predictions))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 1279693.65\nCoefficient of Determination: 0.92\n```\n:::\n:::\n\n\nAs we can see from above, the linear regression model we trained is able to account for 92% of the variance that's explained by the independent variables in our dataset. Which is good, but let us see how we can take that value up while also minimizing the mean squared error using non-linear regression technique like Random Forest Regressor.  \n\n## 3. Random Forests\n\nRandom Forest is an ensemble learning technique. Instead of training a single regressor or classifier, we will train an ensemble of models and choose the prediction that is averaged over all the models. The method used to train this ensemble model is called **bagging**, short for *bootstrap aggregating*. In bagging a single algorithm is chosen as the algorithm that all the models will use but each model will train on a random subset (with replacement) of the overall dataset. Once all the predictors are trained, the ensemble can make the prediction for an instance by simply aggregating the predictions of all the predictors. For regression problems, the aggregation function is usually average. The advantage of such a process is that its decision is based on multiple models rather than a single model - so its calculations are reliable and more accurate than individual predictor. Additionally, the ensemble has lower variance than a single predictor.\n\nLet us train a Random Forest Regressor to predict the diamonds price.  \n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-12-08T02:08:34.805998300Z\",\"start_time\":\"2023-12-08T02:07:57.848591200Z\"}' execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.ensemble import RandomForestRegressor\n\nrfr_reg = make_pipeline(preprocessor, RandomForestRegressor())\nrfr_reg.fit(diamonds_features_train, diamonds_price_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n                 ColumnTransformer(transformers=[(&#x27;num&#x27;,\n                                                  Pipeline(steps=[(&#x27;simpleimputer&#x27;,\n                                                                   SimpleImputer(strategy=&#x27;median&#x27;)),\n                                                                  (&#x27;standardscaler&#x27;,\n                                                                   StandardScaler())]),\n                                                  [&#x27;x&#x27;, &#x27;y&#x27;, &#x27;z&#x27;, &#x27;carat&#x27;,\n                                                   &#x27;depth&#x27;, &#x27;table&#x27;]),\n                                                 (&#x27;cat&#x27;,\n                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n                                                                sparse_output=False),\n                                                  [&#x27;cut&#x27;, &#x27;color&#x27;, &#x27;clarity&#x27;])],\n                                   verbose_feature_names_out=False)),\n                (&#x27;randomforestregressor&#x27;, RandomForestRegressor())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n                 ColumnTransformer(transformers=[(&#x27;num&#x27;,\n                                                  Pipeline(steps=[(&#x27;simpleimputer&#x27;,\n                                                                   SimpleImputer(strategy=&#x27;median&#x27;)),\n                                                                  (&#x27;standardscaler&#x27;,\n                                                                   StandardScaler())]),\n                                                  [&#x27;x&#x27;, &#x27;y&#x27;, &#x27;z&#x27;, &#x27;carat&#x27;,\n                                                   &#x27;depth&#x27;, &#x27;table&#x27;]),\n                                                 (&#x27;cat&#x27;,\n                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n                                                                sparse_output=False),\n                                                  [&#x27;cut&#x27;, &#x27;color&#x27;, &#x27;clarity&#x27;])],\n                                   verbose_feature_names_out=False)),\n                (&#x27;randomforestregressor&#x27;, RandomForestRegressor())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">columntransformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;,\n                                 Pipeline(steps=[(&#x27;simpleimputer&#x27;,\n                                                  SimpleImputer(strategy=&#x27;median&#x27;)),\n                                                 (&#x27;standardscaler&#x27;,\n                                                  StandardScaler())]),\n                                 [&#x27;x&#x27;, &#x27;y&#x27;, &#x27;z&#x27;, &#x27;carat&#x27;, &#x27;depth&#x27;, &#x27;table&#x27;]),\n                                (&#x27;cat&#x27;,\n                                 OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n                                               sparse_output=False),\n                                 [&#x27;cut&#x27;, &#x27;color&#x27;, &#x27;clarity&#x27;])],\n                  verbose_feature_names_out=False)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>[&#x27;x&#x27;, &#x27;y&#x27;, &#x27;z&#x27;, &#x27;carat&#x27;, &#x27;depth&#x27;, &#x27;table&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat</label><div class=\"sk-toggleable__content\"><pre>[&#x27;cut&#x27;, &#x27;color&#x27;, &#x27;clarity&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse_output=False)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div></div></div>\n```\n:::\n:::\n\n\nNow that we trained the RandomForestRegressor, let us make predictions using it and evaluate its performance by calculating mean squared error and r2_score.\n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-12-08T02:25:18.664305200Z\",\"start_time\":\"2023-12-08T02:25:18.327114700Z\"}' execution_count=10}\n``` {.python .cell-code}\nprice_predictor_rfr = rfr_reg.predict(diamonds_features_test)\nprint(\"Mean Squared Error: %.2f\" % mean_squared_error(diamonds_price_test, price_predictor_rfr))\nprint(\"Coefficient of Determination: %.2f\" % r2_score(diamonds_price_test, price_predictor_rfr))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 297054.80\nCoefficient of Determination: 0.98\n```\n:::\n:::\n\n\nGreat! the random forest regressor is able to approximate the actual data much better than the linear regression model we built earlier. Also, the mean squared error is reduced by huge margin. Random Forest is more flexible than linear regression which tries to fit a line to the data while a non-linear regression technique like Decision Trees and Random Forests uses a curve to show association.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}