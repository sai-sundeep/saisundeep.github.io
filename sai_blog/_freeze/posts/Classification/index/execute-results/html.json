{
  "hash": "abb7af461d083a8f5b87ae7294cb87ce",
  "result": {
    "markdown": "---\ntitle: Multiclass Classification on Fashion MNIST Dataset\nauthor: Sai Sundeep Rayidi\ndate: '2023-11-18'\ncategories:\n  - classification\n  - precision\n  - recall\n  - f1-score\n  - code\n  - PCA analysis\n  - Confusion Matrix\n  - RandomizedSearch\n  - visualization\n  - Multi-Class\nimage: classification.png\n---\n\nIn this blog we will be performing multiclass classification on the [Fashion MNIST dataset](https://en.wikipedia.org/wiki/Fashion_MNIST). The dataset contains 60,000 images for training and 10,000 images for validation of 10 different types of fashion products. Multiclass classification is a classification task with more than two classes. Each sample can only be labeled as one class. For example, in the context of fashion-mnist images, each image can be of either a shirt, a sneaker, or a trouser. Each image is one sample and is labeled as one of the 10 possible classes. Multiclass classification makes the assumption that each sample is assigned to one and only one label - one sample cannot, for example, be both a shirt and a coat.\n\n\nWe will be performing the following tasks in this blog - \n* Import, analyze, and visualize the fashion-mnist dataset.\n* Prepare the data for classification \n    + Principal Component Analysis \n    + Hyperparameter Tuning with Randomized Search\n* Train a RandomForestClassifier and visualize its performance metrics\n* Observations and Conclusion\n\nLet us begin by importing a few packages and loading the data into a pandas dataframe. \n\n### 1. Import, analyze, and visualize the fashion-mnist dataset.\n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-11-18T03:34:01.930845300Z\",\"start_time\":\"2023-11-18T03:34:01.921269600Z\"}' execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(category=UserWarning, action='ignore')\n```\n:::\n\n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-11-18T03:34:03.831388700Z\",\"start_time\":\"2023-11-18T03:34:01.930845300Z\"}' execution_count=2}\n``` {.python .cell-code}\n# Load the train and test datasets\nfmnist_train_df = pd.read_csv(\"fashion_mnist_train.csv\")\nfmnist_test_df = pd.read_csv(\"fashion_mnist_test.csv\")\n```\n:::\n\n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-11-18T03:34:03.837791800Z\",\"start_time\":\"2023-11-18T03:34:03.835239200Z\"}' execution_count=3}\n``` {.python .cell-code}\nprint(f'Train Set Dimensions - {fmnist_train_df.shape}')\nprint(f'Test Set Dimensions - {fmnist_test_df.shape}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrain Set Dimensions - (60000, 785)\nTest Set Dimensions - (10000, 785)\n```\n:::\n:::\n\n\nLet us print a few records from the training set and see how the data looks like. \n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-11-18T03:34:03.856649200Z\",\"start_time\":\"2023-11-18T03:34:03.837791800Z\"}' execution_count=4}\n``` {.python .cell-code}\nfmnist_train_df.head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>pixel9</th>\n      <th>...</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n      <th>pixel784</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30</td>\n      <td>43</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>4</td>\n      <td>5</td>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n      <td>...</td>\n      <td>7</td>\n      <td>8</td>\n      <td>7</td>\n      <td>4</td>\n      <td>3</td>\n      <td>7</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>14</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>203</td>\n      <td>214</td>\n      <td>166</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows Ã— 785 columns</p>\n</div>\n```\n:::\n:::\n\n\nWe can see that most of the data is 0 with few values here and there. That is because each row is a 28 * 28 pixel image flattened into an [784,] array. Each element con contain a value between 0 and 255. Furthermore, the target feature named as label is having values from 0 to 9. We will have to process this column a little bit, to make more sense in the context of fashion products that we need to predict. We will see how to do that in just a bit, but first, let us try to reshape the images into 28 * 28 and try to visualize them using the matplotlib subplots.  \n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-11-18T03:34:11.103756500Z\",\"start_time\":\"2023-11-18T03:34:03.856649200Z\"}' execution_count=5}\n``` {.python .cell-code}\n# Visualize the Images in the Dataset\nlabels_list = list(fmnist_train_df['label'].unique())\nlabels_list.sort()\nfig, axs = plt.subplots(10, 10, figsize=(12, 12), layout='constrained')\nfor i in range(0, 10):\n    class_df = fmnist_train_df[fmnist_train_df['label'] == labels_list[i]]\n    for j in range(0, 10):\n        sample_image = class_df.iloc[j, 1:].values.reshape(28, 28)\n        axs[i][j].imshow(sample_image, cmap='Blues')\n        axs[i][j].grid(False)\n        if j == 0: axs[i][j].set_ylabel(f\"Class {labels_list[i]}\")\nplt.grid(False)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=1163 height=1159}\n:::\n:::\n\n\nWe can see from the above plot different kinds of fashion products in the dataset. An obvious difficulty our multiclass classification algorith may face is the close resemblance of three different classes - class 0, class 2, and class 6. Fashion MNIST labels these classes as T-shirt/Top, Pullover, and Shirt respectively. To make target classes clear for us going forward, let us write a helper function that maps, the respective class number to its corresponding label. \n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-11-18T03:34:11.112590300Z\",\"start_time\":\"2023-11-18T03:34:11.103756500Z\"}' execution_count=6}\n``` {.python .cell-code}\n# Create more descriptive labels\ndef label_mapper(input):\n    match input:\n        case 0:\n            return 'T-shirt/top'\n        case 1:\n            return 'Trouser'\n        case 2:\n            return 'Pullover'\n        case 3:\n            return 'Dress'\n        case 4:\n            return 'Coat'\n        case 5:\n            return 'Sandal'\n        case 6:\n            return 'Shirt'\n        case 7:\n            return 'Sneaker'\n        case 8:\n            return 'Bag'\n        case 9:\n            return 'Ankle boot'\n\nfmnist_train_df['label'] = fmnist_train_df['label'].apply(lambda x:label_mapper(x))\nfmnist_test_df['label'] = fmnist_test_df['label'].apply(lambda x:label_mapper(x))\n```\n:::\n\n\nLet us visualize if these changes took effect in the label column by printing a few values.\n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-11-18T03:34:11.118234900Z\",\"start_time\":\"2023-11-18T03:34:11.113138900Z\"}' execution_count=7}\n``` {.python .cell-code}\nfmnist_train_df['label'].head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n0       Pullover\n1     Ankle boot\n2          Shirt\n3    T-shirt/top\n4          Dress\n5           Coat\n6           Coat\n7         Sandal\n8           Coat\n9            Bag\nName: label, dtype: object\n```\n:::\n:::\n\n\n### 2. Prepare the data for classification\n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-11-18T03:34:11.227974300Z\",\"start_time\":\"2023-11-18T03:34:11.118234900Z\"}' execution_count=8}\n``` {.python .cell-code}\n# Split the data into predictors (features) and target (label) in train and test sets\nX_train = fmnist_train_df.iloc[:, 1:]\nX_test = fmnist_test_df.iloc[:, 1:]\ny_train = fmnist_train_df['label'].values\ny_test = fmnist_test_df['label'].values\n```\n:::\n\n\nWe already have the training and test set split for us. So let us just separate the predictor and target variables into matrix X (actually pandas DataFrame) and vector y (a pandas series) from training and test set. We will beusing the X_train and y_train to fit a classification model. We will then be using y_test to make prediction on samples that are unseen by the trained multiclass classification model. Finally, we will use y_test to evaluate our model.\n\nBefore we get ahead to the model, we will need to do some preprocessing. Image classification is a computationally expensive task and training 60,000 images each of 28*28 pixels can take significant of computing resources and training time. Furthermore, we do not have to invest as much resources, because very often only a portion of the images are useful for training. Take for example, in the below image of a pullover, a RandomForestClassifier trained on training data of 1000 samples, showing the important pixels in the image.  \n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-11-18T03:34:13.206759500Z\",\"start_time\":\"2023-11-18T03:34:11.227974300Z\"}' execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.ensemble import RandomForestClassifier\nrnd_clf = RandomForestClassifier(n_estimators=100)\nrnd_clf.fit(X_train[:1000], y_train[:1000])\n\nheatmap_image = rnd_clf.feature_importances_.reshape(28, 28)\nplt.imshow(heatmap_image, cmap=\"hot\")\ncbar = plt.colorbar(ticks=[rnd_clf.feature_importances_.min(),\n                           rnd_clf.feature_importances_.max()])\ncbar.ax.set_yticklabels(['Not important', 'Very important'], fontsize=14)\nplt.axis(\"off\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=582 height=407}\n:::\n:::\n\n\n### 2.1 Principal Component Analysis\n\nAn amazingly helpful strategy to reduce the number of features in the dataset is [Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA). PCA is a dimensionality reduction technique, that transforms the original feature space into a reduced feature space of orthogonal components such that the transformed feature space retains as much variance as possible from the original space. This transformed coordinate system for the principal components. The first principal component can equivalently be defined as a direction that maximizes the variance of the projected data. The i-th principal component can be taken as a direction orthogonal to the first i-1 principal components that maximizes the variance of the projected data. \n\nIt can be shown that the principal components are eigenvectors of the data's covariance matrix. Thus, the principal components are often computed by eigendecomposition of the data covariance matrix or [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) (SVD) of the data matrix. In SVD, we will start from the original data matrix X which has samples in rows and measured variables in columns. The SVD decomposition breaks this matrix X into three matrices - \n\n$$ X(n Ã— p) = U_(n Ã— p) * D_(p Ã— p) * V^T_(p Ã— p) $$ \n\n\nWhere, \n- X contains the original data\n- The columns of U are vectors giving the principal axes. These define the new coordinate system.\n- The scores can be obtained by XV; scores are the projections of the data on the principal axes.\n- D is a diagonal matrix, which means all non-diagonal elements are zero. The diagonal contains positive\nvalues sorted from largest to smallest. These are called the singular values.\n- The columns of V are the PCA loadings\n\nIn addition, U and V are semi-orthogonal matrices, which means that when pre-multiplied by their transpose\none gets the identity matrix:\n\n$$ U^T * U = I $$\n$$ V^T * V = I $$\n\nTo reduce the number of features from 784, We will perform PCA on the fashion-mnist dataset. Before we do that we need to scale our data. PCA assumes that data is normally distributed and is sensitive to the variance of variables, so we scale the data to ensure all variables are on the same scale. This also improves the performance of the algorithm and its prediction accuracy.   \n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-11-18T03:34:20.258302600Z\",\"start_time\":\"2023-11-18T03:34:13.210292300Z\"}' execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\npca = PCA()\npca.fit(X_train)\ncumulative_variance = np.cumsum(pca.explained_variance_ratio_)\nncomponents_95_variance = np.argmax(cumulative_variance >= 0.95) + 1\nprint(f\"Number of Components needed for 95% Variance - {ncomponents_95_variance}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of Components needed for 95% Variance - 256\n```\n:::\n:::\n\n\nTo above step, creates an instance of StandardScaler and fits and transforms the trianing images. The test images are then scaled. The next part performs a full PCA analysis on the training set and calculates the cumulative explained variance - this is the sum of variance by individual principal components. As we can see, to retain 95% of the variance in the dataset, we need just 256 features. It is now computationally manageable for us to train the classifier using a subset of features. One drawback of PCA however is that, it does not tell us which features to retain and which to drop. That is why we will rely on a powerful classifier like **RandomForestClassifier**, which supports **feature sampling.**\n\nBefore that, let me show you a very useful plot that will allow you to visualize the number of components versus the explained variance. This plot allows you to see, how many components you need to retain certain amount of variance in the reduced feature space. For example, in the below plot you can see that, we need only 150 features to explain 91% of the variance in the data. We can build a pretty good model with that variance and strong classifier. \n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-11-18T03:34:20.369456200Z\",\"start_time\":\"2023-11-18T03:34:20.261317800Z\"}' execution_count=11}\n``` {.python .cell-code}\nplt.figure(figsize=(8, 6))\nplt.plot(np.arange(1, len(pca.explained_variance_ratio_)+1, 1), np.cumsum(pca.explained_variance_ratio_)*100)\nplt.plot([150, 150], [0, 91], \"k:\")\nplt.plot([0, 150], [91, 91], \"r:\")\nplt.plot(150, 91, \"ro\")\nplt.text(x=-20, y=91, s='91%', color='red', weight='bold', ha='center', va='center')\nplt.text(x=150, y=-3.5, s='150', color='blue', weight='bold', ha='center', va='center')\nplt.axis([0, 600, 0, 100])\nplt.title(\"Fashion MNIST PCA Analysis - Explained Variance vs # of Components\")\nplt.xlabel(\"Number of Components\", weight='bold')\nplt.ylabel(\"Explained Cumulative Variance (%)\", weight='bold')\nplt.xticks(weight='bold')\nplt.yticks(weight='bold')\nplt.grid(which='major', linestyle='-')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){width=684 height=523}\n:::\n:::\n\n\n### 2.2 Hyperparameter Tuning with Randomized Search\n\nWe can also use hyperparameter tuning to search for best parameters. This is called [Hyperparameter Optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization) or Hyperparameter Tuning. Scikit learn's **GridSearchCV** or **RandomizedSearchCV** can help with hyperparameter tuning. We can get the best values for number of components from principal component analysis and number of estimators from RandomForestClassifier using these methods. RandomizedSearchCV is preferred over GridSearchCV when there are many parameter values to try out. In contrast to GridSearchCV, RandomizedSearchCV tries not all parameter values but rather a fixed number of parameter settings sampled from the specified distributions making the search for best parameters quicker.\n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-11-18T03:42:55.088174200Z\",\"start_time\":\"2023-11-18T03:34:20.371770800Z\"}' execution_count=12}\n``` {.python .cell-code}\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestClassifier\n\npreprocess_pipeline = make_pipeline(PCA(random_state=5805), RandomForestClassifier(random_state=5805))\nparams_distribution = {\n    \"pca__n_components\": np.arange(50, 250), \n    \"randomforestclassifier__n_estimators\": np.arange(100, 400)\n}\nrandomized_search = RandomizedSearchCV(preprocess_pipeline, params_distribution, n_iter=10, cv=3, random_state=5805)\nrandomized_search.fit(X_train[:1000], y_train[:1000])\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n                   estimator=Pipeline(steps=[(&#x27;pca&#x27;, PCA(random_state=5805)),\n                                             (&#x27;randomforestclassifier&#x27;,\n                                              RandomForestClassifier(random_state=5805))]),\n                   param_distributions={&#x27;pca__n_components&#x27;: array([ 50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,\n        63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,\n        76,  77,  78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,\n        89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99,...\n       308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320,\n       321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333,\n       334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346,\n       347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359,\n       360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372,\n       373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385,\n       386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398,\n       399])},\n                   random_state=5805)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3,\n                   estimator=Pipeline(steps=[(&#x27;pca&#x27;, PCA(random_state=5805)),\n                                             (&#x27;randomforestclassifier&#x27;,\n                                              RandomForestClassifier(random_state=5805))]),\n                   param_distributions={&#x27;pca__n_components&#x27;: array([ 50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,\n        63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,\n        76,  77,  78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,\n        89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99,...\n       308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320,\n       321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333,\n       334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346,\n       347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359,\n       360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372,\n       373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385,\n       386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398,\n       399])},\n                   random_state=5805)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(random_state=5805)),\n                (&#x27;randomforestclassifier&#x27;,\n                 RandomForestClassifier(random_state=5805))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(random_state=5805)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=5805)</pre></div></div></div></div></div></div></div></div></div></div></div></div>\n```\n:::\n:::\n\n\nWe can then use the **best_params_** attribute from the model to see the parameters that give best results. Note that above I have performed the search on 1000 training instances. Fell free to try and train on more instances. As you can see below, the number of estimators (desision tree classifiers) required by RandomForestClassifier are 197 and number of features are 234. \n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-11-18T03:42:55.101431400Z\",\"start_time\":\"2023-11-18T03:42:55.088174200Z\"}' execution_count=13}\n``` {.python .cell-code}\nrandomized_search.best_params_\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n{'randomforestclassifier__n_estimators': 197, 'pca__n_components': 234}\n```\n:::\n:::\n\n\n### 3. Train a RandomForestClassifier and visualize its performance metrics\n\nUsing this analysis and hyperparameters, let us now proceed to train the classifier. We will use the n_estimators in RandomForestClassifier to be 197 and fit method to train the classifier. We will use the predict method of the classifier to get the predictions the classifier will make on the dataset. Let us store them in y_pred which we can later use for evaluating the classifier. \n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-11-18T03:43:01.771953300Z\",\"start_time\":\"2023-11-18T03:42:55.095378600Z\"}' execution_count=14}\n``` {.python .cell-code}\n# Perform PCA Transform with above n_components\npca = PCA(n_components=234, random_state=5805)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\n```\n:::\n\n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-11-18T03:51:16.790002500Z\",\"start_time\":\"2023-11-18T03:43:01.771953300Z\"}' execution_count=15}\n``` {.python .cell-code}\n# Train classifier\nrandom_forest_classifier = RandomForestClassifier(n_estimators=197, random_state=5805)\nrandom_forest_classifier.fit(X_train, y_train)\ny_pred = random_forest_classifier.predict(X_test)\n```\n:::\n\n\nNow that we have trained our model and obtained the predictions from it for the test set. It is time to evaluate its performance. A very useful tool for evaluating the multiclass classification model is the **classification_report** from the sklearn metrics module. It gives us the precision, recall, and f1-score for each class. It also gives the overall f1-score of the classifier and a few other metrics we will discuss in just a second. We can extract the classification_report as follows -\n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-11-18T03:51:16.952201200Z\",\"start_time\":\"2023-11-18T03:51:16.793314600Z\"}' execution_count=16}\n``` {.python .cell-code}\nfrom sklearn.metrics import classification_report\n\nprint(\n    f\"Classification report for {random_forest_classifier}:\\n\"\n    f\"{classification_report(y_test, y_pred)}\\n\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nClassification report for RandomForestClassifier(n_estimators=197, random_state=5805):\n              precision    recall  f1-score   support\n\n  Ankle boot       0.89      0.94      0.92      1000\n         Bag       0.93      0.97      0.95      1000\n        Coat       0.79      0.87      0.83      1000\n       Dress       0.88      0.92      0.90      1000\n    Pullover       0.80      0.79      0.80      1000\n      Sandal       0.93      0.90      0.92      1000\n       Shirt       0.74      0.57      0.65      1000\n     Sneaker       0.90      0.88      0.89      1000\n T-shirt/top       0.79      0.84      0.82      1000\n     Trouser       0.99      0.96      0.98      1000\n\n    accuracy                           0.87     10000\n   macro avg       0.86      0.87      0.86     10000\nweighted avg       0.86      0.87      0.86     10000\n\n\n```\n:::\n:::\n\n\nSo how do we read this classification report? Let us first define each of the metrics. \n\n - **Precision** - In a multi-class classification task, the precision for a class is the number of true positives (i.e. the number of images correctly labelled as belonging to the positive class) divided by the total number of elements labelled as belonging to the positive class (i.e. the sum of true positives and false positives, which are images incorrectly labelled as belonging to the class).\n\n$$ {\\displaystyle \\mathrm {PPV} ={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FP} }}=1-\\mathrm {FDR} } $$\n\n - **Recall** - Recall in this context is defined as the number of true positives divided by the total number of images that actually belong to the positive class (i.e. the sum of true positives and false negatives, which are images which were not labelled as belonging to the positive class but should have been).\n\n$$ {\\displaystyle \\mathrm {TPR} ={\\frac {\\mathrm {TP} }{\\mathrm {P} }}={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FN} }}=1-\\mathrm {FNR} } $$\n\n > Precision and recall are not particularly useful metrics when used in isolation. The need to be used in conjunction with other metrics to obtain a comprehensive evaluation of the modelâ€™s performance. For instance, A trivial way to have perfect precision is to create a classifier that always makes negative predictions, except for one single positive prediction on the instance itâ€™s most confident about. If this one prediction is correct, then the classifier has 100% precision. Such a classifer would not be any helpful. Likewise, it is possible to have perfect recall by simply retrieving every single item. \n - **F1-score and Accuracy** - The F1-score is a metric that combines precision and recall to obtain a balanced classification model. The F1 score favors classifiers that have similar precision and recall. It is calculated as the harmonic mean of precision and recall. Accuracy is another metric that measures the proportion of correct predictions made by the model.\n\n**F1 Score**\n$$ {\\displaystyle \\mathrm {F} _{1}=2\\times {\\frac {\\mathrm {PPV} \\times \\mathrm {TPR} }{\\mathrm {PPV} +\\mathrm {TPR} }}={\\frac {2\\mathrm {TP} }{2\\mathrm {TP} +\\mathrm {FP} +\\mathrm {FN} }}} $$\n\n**Accuracy (ACC)**\n$$ {\\displaystyle \\mathrm {ACC} ={\\frac {\\mathrm {TP} +\\mathrm {TN} }{\\mathrm {P} +\\mathrm {N} }}={\\frac {\\mathrm {TP} +\\mathrm {TN} }{\\mathrm {TP} +\\mathrm {TN} +\\mathrm {FP} +\\mathrm {FN} }}} $$\n\nAnathor important tool to evaluate the classifier is the **Confusion Matrix**, in the multi-class classification context, it gives the actual predictions in horizontal rows and predicted classes in vertical columns. We can use the ConfusionMatrixDisplay from scikit learn's metrics module to plot the confusion matrix for the classifier.  \n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-11-18T03:57:37.274586600Z\",\"start_time\":\"2023-11-18T03:57:37.080311300Z\"}' execution_count=17}\n``` {.python .cell-code}\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\ndisp = ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\ndisp.figure_.suptitle(\"Confusion Matrix\")\nprint(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\nplt.grid(False)\nplt.xticks(rotation=60)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion matrix:\n[[944   0   0   0   0  20   0  36   0   0]\n [  0 975   4   4   4   2   7   2   2   0]\n [  0   8 868  24  51   0  49   0   0   0]\n [  0   3  24 922   8   0  14   0  24   5]\n [  0  13 117  10 788   1  57   0  14   0]\n [ 36   6   0   0   0 902   0  56   0   0]\n [  0  25  82  23 113   0 575   0 182   0]\n [ 76   1   0   0   0  41   0 882   0   0]\n [  1  20   4  40  13   5  71   1 844   1]\n [  1   1   3  20   4   0   4   0   3 964]]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-18-output-2.png){width=567 height=533}\n:::\n:::\n\n\nWe can normalize the confusion matrix by dividing each value by the total number of images in the corresponding (true) class (i.e., divide by the rowâ€™s sum). This helps us to see where the model has misclassfied the images of a given class as belonging to different class. \n\n::: {.cell ExecuteTime='{\"end_time\":\"2023-11-18T03:59:13.797141700Z\",\"start_time\":\"2023-11-18T03:59:13.480594700Z\"}' execution_count=18}\n``` {.python .cell-code}\n# Plot the normalized confused matrix.\nConfusionMatrixDisplay.from_predictions(y_test, y_pred, normalize='true', values_format=\".0%\")\nplt.grid()\nplt.title(\"Normalized Confusion Matrix\")\nplt.xticks(rotation=60)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-19-output-1.png){width=563 height=505}\n:::\n:::\n\n\n### 4. Observations and Conclusion\n\nLooking at the classification report, ROC Curve, and confusion matrix, we can make following observations -\n\nThe precision for \"coat\", \"shirt\", \"pullover\", and \"T-shirt/top\" is very low. This tells us that the model is predicting many False Positives for these classes. This is because these four classes are closely resembling one another. The classifier is unable to tell apart confidently a \"coat\" from a \"shirt\" and is resulting in many false positives and low precision. One way to improve the model precision for these labels is to increase the number of samples so that model can learn the subtle differences to tell apart each of these classes.\n\nThe recall for shirt is very low 57%. This tells that many images of shirt have been misclassified as belonging to a different class. Looking at the normalized confusin matrix, we can see that 18% of shirts have been misclassified as T-Shirt/Top, 11% of the shirts have been misclassified as pullover and 8% as Coats. We can see the same issue with images of pullover and coat, they have been misclassified as these other similar looking products.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}